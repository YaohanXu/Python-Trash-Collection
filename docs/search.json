[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Team Members",
    "section": "",
    "text": "Name: Neve Zhang\nConcentration: Smart Cities\nContact Information: tianyuzh@upenn.edu\nIntroduction: I’m a current Master of City Planning student at the University of Pennsylvania, graduating in 2025. I have expertise in data analytics, technical writing, and market and policy research. As a nature lover and global traveler, I’m passionate about combining sustainability and data-driven innovation to shape better urban environments.\nHobbies:\n\nBadminton\n\n\n\n\n\n\n\n\nName: Yaohan Xu\nConcentration: Sustainable Transportation and Infrastructure Planning (STIP)\nContact Information: xuyaohan@upenn.edu\nIntroduction: I specialize in city and transportation planning, focusing on data-driven solutions. Leveraging expertise in product and interaction design, I develop intuitive digital platforms to improve urban living and connectivity.\nHobbies:\n\nTennis\nCooking",
    "crumbs": [
      "Team Members"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Optimizing Trash Collection Schedules",
    "section": "",
    "text": "As a metropolitan home to over 8 million population, New York City is known for its wary sanitary conditions due to frequent street littering. For public agencies like the New York City Department of Sanitation (DSNY), which is responsible for residential trash, recycling, and street cleaning, there remain vast challenges to delivering trash management services, especially in underrepresented neighborhoods with limited local capacity.\nLeveraging 311 Service Requests, DSNY Frequencies, ACS (socioeconomic characteristics), and other spatial/time series data such as restaurant inspection records, we seek to investigate the patterns of sanitary complaints (ie. litter baskets, cleaning streets) across DSNY boundaries and days of the week, and devising a prediction algorithm to facilitate DSNY’s future trash collection schedules across the agency’s 609 frequency boundaries.",
    "crumbs": [
      "Project Overview"
    ]
  },
  {
    "objectID": "index.html#here-is-a-brief-introduction-of-our-topic",
    "href": "index.html#here-is-a-brief-introduction-of-our-topic",
    "title": "Optimizing Trash Collection Schedules",
    "section": "",
    "text": "As a metropolitan home to over 8 million population, New York City is known for its wary sanitary conditions due to frequent street littering. For public agencies like the New York City Department of Sanitation (DSNY), which is responsible for residential trash, recycling, and street cleaning, there remain vast challenges to delivering trash management services, especially in underrepresented neighborhoods with limited local capacity.\nLeveraging 311 Service Requests, DSNY Frequencies, ACS (socioeconomic characteristics), and other spatial/time series data such as restaurant inspection records, we seek to investigate the patterns of sanitary complaints (ie. litter baskets, cleaning streets) across DSNY boundaries and days of the week, and devising a prediction algorithm to facilitate DSNY’s future trash collection schedules across the agency’s 609 frequency boundaries.",
    "crumbs": [
      "Project Overview"
    ]
  },
  {
    "objectID": "analysis/0b-trim-additional-data.html",
    "href": "analysis/0b-trim-additional-data.html",
    "title": "Trim Additional Data",
    "section": "",
    "text": "Do not Re-run!!\nTo enhance the efficiency of data analysis, this section is used to pre-process other large datasets and store them as clean, reduced datasets created for analysis purposes. Additional data gathered and cleaned include Department of Health and Mental Hygiene (DOHMH) New York City Restaurant Inspection Results and additional 311 request records.\nThis section should not be re-run due to its sole purpose for the creation of more manageable datasets.\nsource: - DOHMH New York City Restaurant Inspection Results, NYC Open Data [https://data.cityofnewyork.us/Health/DOHMH-New-York-City-Restaurant-Inspection-Results/43nn-pn8j/about_data] - 311 Service Requests from 2010 to Present, NYC Open Data [https://data.cityofnewyork.us/Social-Services/311-Service-Requests-from-2010-to-Present/erm2-nwe9/about_data]",
    "crumbs": [
      "Data Analysis",
      "Trim Additional Data"
    ]
  },
  {
    "objectID": "analysis/0b-trim-additional-data.html#set-up",
    "href": "analysis/0b-trim-additional-data.html#set-up",
    "title": "Trim Additional Data",
    "section": "1 Set Up",
    "text": "1 Set Up\n\nimport pandas as pd\nimport geopandas as gpd\nimport numpy as np\n\nfrom datetime import datetime",
    "crumbs": [
      "Data Analysis",
      "Trim Additional Data"
    ]
  },
  {
    "objectID": "analysis/0b-trim-additional-data.html#dohmh-new-york-city-restaurant-inspection-results",
    "href": "analysis/0b-trim-additional-data.html#dohmh-new-york-city-restaurant-inspection-results",
    "title": "Trim Additional Data",
    "section": "2 DOHMH New York City Restaurant Inspection Results",
    "text": "2 DOHMH New York City Restaurant Inspection Results\nFirst, we loaded the New York City Restaurant Results dataset and extracted all unique violation types. While the violation types are highly descriptive, we note that terms like “mice,” “roaches,” and “vermin” are indicative of a restaurant’s high vulnerability to trash mismanagement. Using a complete list of these indicative words, we filtered the dataset to include only trash-related conditions and stored it for further use.\n\nres_inspection = pd.read_csv(\"data/DOHMH_New_York_City_Restaurant_Inspection_Results.csv\")\n\n\nviolation_types = res_inspection['VIOLATION DESCRIPTION'].unique()\n\nprint(violation_types)\n\n[nan 'Thawing procedure improper.'\n 'Hot TCS food item not held at or above 140 °F.'\n 'Hot food item not held at or above 140º F.'\n \"Evidence of mice or live mice in establishment's food or non-food areas.\"\n 'Hand wash sign not posted'\n \"Live roaches in facility's food or non-food area.\"\n 'Current letter grade or Grade Pending card not posted'\n 'Current letter grade sign not posted.'\n 'Food contact surface not properly maintained.'\n 'Wiping cloths soiled or not stored in sanitizing solution.'\n 'Ashtrays in smoke-free area' 'Thawing procedures improper.'\n 'Wiping cloths not stored clean and dry, or in a sanitizing solution, between uses.'\n 'Permit not conspicuously displayed.'\n 'Equipment used for ROP not approved by the Department'\n 'Wash hands sign not posted near or above hand washing sink.'\n \"Live roaches present in facility's food and/or non-food areas.\"\n \"Evidence of rats or live rats in establishment's food or non-food areas.\"\n 'MISBRANDED AND LABELING'\n 'Workplace SFAA policy not prominently posted in workplace.'\n 'Proper sanitization not provided for utensil ware washing operation.'\n 'Workplace SFAA policy not prominently posted in workplace'\n \"Evidence of mice or live mice present in facility's food and/or non-food areas.\"\n 'Food worker/food vendor does not use utensil or other barrier to eliminate bare hand contact with food that will not receive adequate additional heat treatment.'\n 'Food Protection Certificate not held by supervisor of food operations.'\n 'Sewage disposal system is not provided, improper, inadequate or unapproved.'\n 'Failure to display required signage about plastic straw availability'\n 'Establishment is not free of harborage or conditions conducive to rodents, insects or other pests.'\n 'Lighting Inadequate'\n 'Failure to post or conspicuously post healthy eating information'\n 'Food Protection Certificate (FPC) not held by manager or supervisor of food operations.'\n 'No Smoking permitted in an area where it is prohibited'\n 'Sewage disposal system improper or unapproved.'\n 'Accurate thermometer not provided in refrigerated or hot holding equipment.'\n 'Food Protection Certificate not available for inspection'\n 'Failure to display required signage about plastic straw availability.'\n 'Non-food contact surface or equipment made of unacceptable material, not kept clean, or not properly sealed, raised, spaced or movable to allow accessibility for cleaning on all sides, above and underneath the unit.'\n 'Accurate thermometer not provided or properly located in refrigerated, cold storage or hot holding equipment'\n 'Raw, cooked or prepared food is adulterated, contaminated, cross-contaminated, or not discarded in accordance with HACCP plan.'\n 'After cooking or removal from hot holding, TCS food not cooled by an approved method whereby the internal temperature is reduced from 140 °F to 70 °F or less within 2  hours, and from 70 °F to 41 °F or less within 4 additional hours.'\n 'Insufficient or no refrigerated or hot holding equipment to keep potentially hazardous foods at required temperatures.'\n 'Design, construction, materials used or maintenance of food contact surface improper.  Surface not easily cleanable, sanitized and maintained.'\n 'Non-food contact surface improperly constructed. Unacceptable material used. Non-food contact surface or equipment improperly maintained and/or not properly sealed, raised, spaced or movable to allow accessibility for cleaning on all sides, above and underneath the unit.'\n 'Sanitized equipment or utensil, including in-use food dispensing utensil, improperly used or stored.'\n 'Cold TCS food item held above 41 °F; smoked or processed fish held above 38 °F; intact raw eggs held above 45 °F; or reduced oxygen packaged (ROP) TCS foods held above required temperatures except during active necessary preparation.'\n 'Facility not vermin proof. Harborage or conditions conducive to attracting vermin to the premises and/or allowing vermin to exist.'\n 'Filth flies or food/refuse/sewage associated with (FRSA) flies or other nuisance pests in establishment’s food and/or non-food areas. FRSA flies include house flies, blow flies, bottle flies, flesh flies, drain flies, Phorid flies and fruit flies.'\n 'Single service article not provided.  Single service article reused or not protected from contamination when transported, stored, dispensed.  Drinking straws not completely enclosed in wrapper or dispensed from a sanitary device.'\n 'Mechanical or natural ventilation not provided, inadequate, improperly installed, in disrepair or fails to prevent and control excessive build-up of grease, heat, steam condensation, vapors, odors, smoke or fumes.'\n 'Food contact surface not properly washed, rinsed and sanitized after each use and following any activity when contamination may have occurred.'\n 'Design, construction, materials used or maintenance of food contact surface improper. Surface not easily cleanable, sanitized and maintained.'\n 'Design, construction, materials used or maintenance of food contact surface improper.  Surface not easily cleanable, sanitized and maintained.  Culinary sink or other acceptable method not provided for washing food.'\n 'Food not protected from potential source of contamination during storage, preparation, transportation, display or service.'\n 'Filth flies or food/refuse/sewage associated with (FRSA) flies or other nuisance pests  in  establishment’s food and/or non-food areas. FRSA flies include house flies, blow flies, bottle flies, flesh flies, drain flies, Phorid flies and fruit flies.'\n 'Plumbing not properly installed or maintained; anti-siphonage or backflow prevention device not provided where required; equipment or floor not properly drained; sewage disposal system in disrepair or not functioning properly.'\n 'Anti-siphonage or back-flow prevention device not provided where required; equipment or floor not properly drained; sewage disposal system in disrepair or not functioning properly. Condensation or liquid waste improperly disposed of.'\n 'Food, supplies, and equipment not protected from potential source of contamination during storage, preparation, transportation, display or service.'\n 'Swollen, leaking, rusted or otherwise damaged canned food to be returned to distributor not segregated from intact product and clearly labeled DO NOT USE'\n 'Personal cleanliness inadequate. Outer garment soiled with possible contaminant. Effective hair restraint not worn in an area where food is prepared.'\n 'Dishwashing and ware washing: Cleaning and sanitizing of tableware, including dishes, utensils, and equipment deficient.'\n 'Personal cleanliness is inadequate. Outer garment soiled with possible contaminant. Effective hair restraint not worn where required. Jewelry worn on hands or arms. Fingernail polish worn or fingernails not kept clean and trimmed.'\n 'Food, supplies, or equipment not protected from potential source of contamination during storage, preparation, transportation, display, service or from customer’s refillable, reusable container.  Condiments not in single-service containers or dispensed directly by the vendor.'\n 'Contract with a pest management professional not in place.  Record of extermination activities not kept on premises.'\n 'No hand washing facility in or adjacent to toilet room or within 25 feet of a food preparation, food service or ware washing area. Hand washing facility not accessible, obstructed or used for non-hand washing purposes. No hot and cold running water or water at inadequate pressure. No soap or acceptable hand-drying device.'\n 'Properly scaled and calibrated thermometer or thermocouple not provided or not readily accessible in food preparation and hot/cold holding areas to measure temperatures of TCS foods during cooking, cooling, reheating, and holding.'\n 'Pesticide not properly labeled or used by unlicensed individual. Pesticide, other toxic chemical improperly used/stored. Unprotected, unlocked bait station used.'\n 'Mechanical or natural ventilation system not provided,  improperly installed, in disrepair and/or fails to prevent excessive build-up of grease, heat, steam condensation vapors, odors, smoke, and fumes.'\n 'Cold food item held above 41º F (smoked fish and reduced oxygen packaged foods above 38 ºF) except during necessary preparation.'\n \"Evidence of rats or live rats present in facility's food and/or non-food areas.\"\n 'Personal cleanliness is inadequate. Outer garment soiled with possible contaminant. Effective hair restraint not worn where required.  Jewelry is worn on hands or arms.  Fingernail polish worn or fingernails not kept clean and trimmed.'\n 'Toilet facility not provided for employees or for patrons when required. Shared patron-employee toilet accessed through kitchen, food prep or storage area or utensil washing area.'\n 'No approved written standard operating procedure for avoiding contamination by refillable returnable containers.'\n 'Single service item reused, improperly stored, dispensed; not used when required.'\n 'Garbage receptacle not pest or water resistant, with tight-fitting lids, and covered except while in active use. Garbage receptacle and cover not cleaned after emptying and prior to reuse. Garbage, refuse and other solid and liquid waste not collected, stored, removed and disposed of so as to prevent a nuisance.'\n 'Food allergy information poster not conspicuously posted where food is being prepared or processed by food workers.'\n 'The original nutrition fact labels or ingredient label for a cooking oil, shortening or margarine or food item sold in bulk, or acceptable manufacturer’s documentation not maintained on site.'\n 'Food worker/food vendor does not wash hands thoroughly after using the toilet, or after coughing, sneezing, smoking, eating, preparing raw foods or otherwise contaminating hands or does not change gloves when required; Worker fails to refrain from smoking or being fully clothed in clean outer garments.'\n 'Contract with a pest management professional not in place. Record of extermination activities not kept on premises.'\n 'Food, supplies, or equipment not protected from potential source of contamination during storage, preparation, transportation, display, service or from customer’s refillable, reusable container. Condiments not in single-service containers or dispensed directly by the vendor.'\n 'Insufficient or no hot holding, cold storage or cold holding equipment provided to maintain Time/Temperature Control for Safety Foods (TCS) at required temperatures'\n 'Tobacco or electronic cigarette use, eating, or drinking from open container in food preparation, food storage or dishwashing area.'\n 'Unapproved outdoor, street or sidewalk cooking.'\n 'Filth flies or food/refuse/sewage-associated (FRSA) flies present in facility’s food and/or non-food areas.  Filth flies include house flies, little house flies, blow flies, bottle flies and flesh flies.  Food/refuse/sewage-associated flies include fruit flies, drain flies and Phorid flies.'\n 'TCS food removed from cold holding or prepared from or combined with ingredients at room temperature not cooled by an approved method to 41 °F or below within 4 hours.'\n 'Pesticide use not in accordance with label or applicable laws. Prohibited chemical used/stored. Open bait station used.'\n 'After cooking or removal from hot holding, TCS food not cooled by an approved method whereby the internal temperature is reduced from 140 °F to 70 °F or less within 2 hours, and from 70 °F to 41 °F or less within 4 additional hours.'\n '“Choking first aid” poster not posted. “Alcohol and Pregnancy” warning sign not posted. Resuscitation equipment: exhaled air resuscitation masks (adult & pediatric), latex gloves, sign not posted.'\n \"Live animals other than fish in tank or service animal present in facility's food and/or non-food areas.\"\n 'Pesticide not properly labeled or used by unlicensed individual.  Pesticide, other toxic chemical improperly used/stored. Unprotected, unlocked bait station used.'\n 'Toilet facility not maintained or provided with toilet paper, waste receptacle or self-closing door.'\n 'Hand washing facility not provided in or near food preparation area and toilet room. Hot and cold running water at adequate pressure to enable cleanliness of employees not provided at facility. Soap and an acceptable hand-drying device not provided.'\n 'Food not cooled by an approved method whereby the internal product temperature is reduced from 140º F to 70º F or less within 2  hours, and from 70º F to 41º F or less within 4 additional hours.'\n 'Dishwashing and ware washing:  Cleaning and sanitizing of tableware, including dishes, utensils, and equipment deficient.'\n 'Live animal other than fish in tank or service animal present in facility’s food or non-food area.'\n 'HACCP plan not approved or approved HACCP plan not maintained on premises.'\n 'Food, food preparation area, food storage area, area used by employees or patrons, contaminated by sewage or liquid waste.'\n 'Warning sign re dangers of hookah smoking not posted at or on each public entryway to Non-Tobacco Hookah Establishment (NTHE) patron'\n 'Failure to maintain a sufficient supply of single-use, non-compostable plastic straws.'\n 'Sign prohibiting smoking or using electronic cigarettes not conspicuously posted.'\n 'Food worker does not use proper utensil to eliminate bare hand contact with food that will not receive adequate additional heat treatment.'\n 'Nuisance created or allowed to exist. Facility not free from unsafe, hazardous, offensive or annoying condition.'\n 'Single service article not provided. Single service article reused or not protected from contamination when transported, stored, dispensed. Drinking straws not completely enclosed in wrapper or dispensed from a sanitary device.'\n 'Required succinct nutritional statements not posted on menu(s) for adults and children (2,000 calories per day for adults)'\n 'Flavored tobacco products sold or offered for sale'\n 'No hand washing facility in or adjacent to toilet room or within 25 feet of a food preparation, food service or ware washing area.  Hand washing facility not accessible, obstructed or used for non-hand washing purposes. No hot and cold running water or water at inadequate pressure. No soap or acceptable hand-drying device.'\n 'Providing single-use, non-compostable plastic straws to customers without customer request (including providing such straws at a self-serve station)'\n 'Failure to comply with an order of the Board of Health, Commissioner or Department.'\n 'Additional nutritional information statement not posted, or additional nutritional information not provided'\n 'Current letter grade or \"Grade Pending\" card not posted.'\n 'Shellfish not from approved source, not or improperly tagged/labeled; tags not retained for 90 days.'\n 'Tobacco use, eating, or drinking from open container in food preparation, food storage or dishwashing area observed.'\n 'Food, prohibited, from unapproved or unknown source, home canned or home prepared. Animal slaughtered, butchered or dressed (eviscerated, skinned) in establishment. Reduced Oxygen Packaged (ROP) fish not frozen before processing. ROP food prepared on premises transported to another site.'\n 'Personal cleanliness is inadequate. Outer garment soiled with possible contaminant. Effective hair restraint not worn where required.  Jewelry worn on hands or arms.  Fingernail polish worn or fingernails not kept clean and trimmed.'\n 'No or inadequate potable water supply. Water or ice not potable or from unapproved source. Bottled water not NY State certified. Cross connection in potable water supply system.'\n 'Additional nutritional information statement not posted, and/or additional nutritional information not provided.'\n 'Caloric content not posted on menus, menu boards or food tags, in a food service establishment that is 1 of 15 or more outlets operating the same type of business nationally under common ownership or control, or as a franchise or doing business under the same name, for each menu item that is served in portions, the size and content of which are standardized.'\n 'Lighting fixture located over, by or within food storage, preparation, service or display facility, and facility where utensils and equipment are cleaned and stored, which may shatter due to extreme heat, temperature changes or accidental contact; not fitted with shatterproof bulb or shielded and encased, with end caps or other device.'\n 'Harmful, noxious gas or vapor detected. CO =13 ppm.'\n 'Food adulterated or misbranded.  Adulterated or misbranded food possessed, being manufactured, produced, packed, sold, offered for sale, delivered or given away'\n 'Document issued by the Board of Health, Commissioner or Department reproduced or altered. False, untrue or misleading statement or document made to, submitted or filed with the Department'\n 'Time/Temperature Control for Safety (TCS) food not cooked to required minimum internal temperature.   • Poultry, poultry parts, ground and comminuted poultry, all stuffing containing poultry, meats, fish or ratites to or above 165 °F for 15 seconds with no interruption of the cooking process   • Ground meat, and food containing ground and comminuted meat, to or above 158 °F for 15 seconds with no interruption of the cooking process, except per individual customer request  • Pork, any food containing pork to or above 150 °F for 15 seconds  • Mechanically tenderized or injected meats to or above 155 °F.  • Whole meat roasts and beef steak to or above required temperature and time except per individual customer request  • Raw animal foods cooked in microwave to or above165 °F, covered, rotated or stirred   • All other foods to or above 140 °F for 15 seconds; shell eggs to or above 145 °F for 15 seconds except per individual customer request.'\n 'Appropriately scaled metal stem-type thermometer or thermocouple not provided or used to evaluate temperatures of potentially hazardous foods during cooking, cooling, reheating and holding.'\n 'Current letter grade or \"Grade Pending\" card not conspicuously posted or visible to passersby'\n 'Required calorie information not posted/improperly posted.'\n 'Unclean or cracked whole eggs or unpasteurized liquid, frozen or powdered eggs kept or used.'\n 'Covered garbage receptacle not provided or inadequate, except that garbage receptacle may be uncovered during active use. Garbage storage area not properly constructed or maintained; grinder or compactor dirty.'\n 'Food worker does not wash hands thoroughly after using the toilet, coughing, sneezing, smoking, eating, preparing raw foods or otherwise contaminating hands.'\n 'Lighting inadequate; permanent lighting not provided in food preparation areas, ware washing areas, and storage rooms. Shatterproof bulb or shield to prevent broken glass from falling into food or onto surfaces, not installed.'\n '“Choking first aid” poster not posted. “Alcohol and pregnancy” warning sign not posted. Resuscitation equipment: exhaled air resuscitation masks (adult & pediatric), latex gloves, sign not posted.'\n 'Hot TCS food item that has been cooked and cooled is being held for service without first being reheated to 165º F or above within 2 hours.'\n 'Canned food product observed dented and not segregated from other consumable food items.'\n 'Records and logs not maintained to show that approved HACCP plan has been properly implemented.'\n 'No facilities available to wash, rinse and sanitize utensils and/or equipment.'\n 'Harmful noxious gas or vapor detected. Carbon Monoxide (CO) level exceeds nine (9) ppm'\n 'Juice packaged on premises with no or incomplete label, no warning statement'\n 'Food prepared from ingredients at ambient temperature not cooled to 41º F or below within 4 hours.'\n 'Meat, fish, molluscan shellfish, unpasteurized raw shell eggs, poultry or other TCS offered or served raw or undercooked and written notice not provided to consumer.'\n 'Hot food item that has been cooked and refrigerated is being held for service without first being reheated to 165º F or above within 2 hours.'\n 'Toxic chemical improperly labeled, stored or used such that food contamination may occur.'\n 'Nuisance created or allowed to exist.  Facility not free from unsafe, hazardous, offensive or annoying conditions.'\n 'Sale or use of certain expanded polystyrene items restricted'\n 'Providing single-use plastic stirrers or single-use plastic splash sticks.'\n 'Food worker/food vendor does not wash hands thoroughly  after using the toilet, or after coughing, sneezing, smoking, eating, preparing raw foods or otherwise contaminating hands or does not change gloves when required; Worker fails to refrain from smoking or  being fully clothed in clean outer garments.'\n 'Food not cooked to required minimum temperature.'\n 'Food from unapproved or unknown source or home canned. Reduced oxygen packaged (ROP) fish not frozen before processing; or ROP foods prepared on premises transported to another site.'\n 'Food adulterated or misbranded. Adulterated or misbranded food possessed, being manufactured, produced, packed, sold, offered for sale, delivered or given away'\n 'Hot TCS food item that has been cooked and  cooled is being held for service without first being reheated to 165º F or above within 2 hours.'\n 'Toxic chemical or pesticide improperly stored or used such that food contamination may occur.'\n 'Expanded Polystyrene (EPS) single service article not designated as a recyclable material.'\n 'Bulb not shielded or shatterproof, in areas where there is extreme heat, temperature changes, or where accidental contact may occur.'\n 'Food preparation area, food storage area, or other area used by employees or patrons, contaminated by sewage or liquid waste.'\n 'Food contact surface, refillable, reusable containers, or equipment improperly constructed, placed or maintained. Unacceptable material used. Culinary sink or other acceptable method not provided for washing food.'\n 'Food contact surface, refillable, reusable containers, or equipment improperly constructed, placed or maintained. Unacceptable material used.  Culinary sink or other acceptable method not provided for washing food.'\n 'Commercially processed pre-cooked TCS in hermetically sealed containers and precooked TCS in intact packages from non-retail food processing establishments not heated to 140 °F within 2 hours of removal from container or package.'\n 'Precooked potentially hazardous food from commercial food processing establishment that is supposed to be heated, but is not heated to 140º F within 2 hours.'\n 'Toilet facility not maintained and provided with toilet paper, waste receptacle and self-closing door.'\n 'Required nutritional information statement not posted (2,000 calories per day).'\n 'Providing compostable plastic straws to be used outside of the food establishment’s premises; failure to appropriately dispose of compostable plastic straws; failure to maintain required bins for disposal of compostable plastic straws.'\n 'Food, prohibited, from unapproved or unknown source, home canned or home prepared.  Animal slaughtered, butchered or dressed (eviscerated, skinned) in establishment. Reduced Oxygen Packaged (ROP) fish not frozen before processing.  ROP food prepared on premises transported to another site.'\n 'The original nutritional fact labels and/or ingredient label for a cooking oil, shortening or margarine or food item sold in bulk, or acceptable manufacturer’s documentation not maintained on site.'\n 'Current valid permit, registration or other authorization to operate a Temporary Food Service Establishment (TFSE) not available.'\n 'Food contact surface improperly constructed or located. Unacceptable material used.'\n 'Food service operation occurring in room or area used as living or sleeping quarters.'\n 'Food containing a prohibited substance held, kept, offered, prepared, processed, packaged, or served.'\n 'Manufacture of frozen dessert not authorized on Food Service Establishment permit. Milk or milk product undated, improperly dated or expired.'\n 'Written Standard Operating Procedure (SOP) approved by the Department for refillable, reusable containers not available at the time of inspection.  Container construction improper'\n 'Time/Temperature Control for Safety (TCS) food not cooked to required minimum internal temperature. • Poultry, poultry parts, ground and comminuted poultry, all stuffing containing poultry, meats, fish or ratites to or above 165 °F for 15 seconds with no interruption of the cooking process  • Ground meat, and food containing ground and comminuted meat, to or above 158 °F for 15 seconds with no interruption of the cooking process, except per individual customer request• Pork, any food containing pork to or above 150 °F for 15 seconds• Mechanically tenderized or injected meats to or above 155 °F.• Whole meat roasts and beef steak to or above required temperature and time except per individual customer request• Raw animal foods cooked in microwave to or above165 °F, covered, rotated or stirred  • All other foods to or above 140 °F for 15 seconds; shell eggs to or above 145 °F for 15 seconds except per individual customer request.'\n 'Harmful noxious gas or vapor detected.  Carbon Monoxide (CO) level exceeds nine (9) ppm'\n 'Food, food preparation area, food storage area, or other area used by employees or patrons, contaminated by sewage or liquid waste.'\n 'Flavored tobacco products sold, offered for sale'\n 'Sodium Warning statement not posted conspicuously at the point of purchase. “Warning: [icon image] indicates that the sodium (salt) content of this item is higher than the total daily recommended limit (2300 mg). High sodium intake can increase blood pressure and risk of heart disease and stroke.”'\n 'Food worker or vendor working or is knowingly or negligently permitted to work in FSE while afflicted with  infected wound  or reportable communicable disease.  No spitting allowed. Spitting anywhere in the establishment is prohibited.'\n 'Sodium Warning icon not posted on menus, menu boards or food tags for food items that contain 2300mg or more of sodium in a food service establishment that is 1 of 15 or more outlets operating the same type of business nationally under common ownership or control, or as a franchise or doing business under the same name, for each menu item that is served in portions, the size and content of which are standardized'\n 'Order or notice posted or required to be posted by the Department mutilated, obstructed or removed.'\n 'Smoke free workplace smoking policy inadequate, not posted, not provided to employees.'\n 'Food package (including canned food product or hermetically sealed container) not in good condition to the degree contents are not suitable for human consumption. Package is swollen, leaking or rusted, without “DO NOT USE” label or not segregated from consumable food items.'\n 'Original label for smoking products sold or offered for sale.'\n 'Current valid permit, registration or other authorization to operate a Food Service Establishment (FSE) or Non-retail Food Processing Establishment (NRFP) not available.'\n 'Ashtray present in smoke-free area.'\n 'Failure to make a good faith effort to inform smokers or electronic cigarette users of Smoke-Free Air Act (\"SFAA\") prohibitions'\n 'Failure to provide a single-use, non-compostable plastic straw upon request.'\n 'Sodium Warning - Statement'\n 'Document issued by the Board of Health, Commissioner or Department unlawfully reproduced or altered.'\n 'Food service operation occurring in room used as living or sleeping quarters.'\n 'Potable water supply inadequate. Water or ice not potable or from unapproved source. Cross connection in potable water supply system observed.'\n 'Operator failed to make a good faith effort to inform smokers or users of electronic cigarettes of the SFAA'\n 'ROP processing equipment not approved by DOHMH.'\n 'Smoking or electronic cigarette use allowed in prohibited area'\n 'Failure to comply with an Order of the Board of Health, Commissioner, or Department.'\n 'No or inadequate potable water supply. Water or ice not potable or from unapproved source. Bottled water not NY State certified.  Cross connection in potable water supply system.'\n 'Food allergy poster does not contain text provided or approved by Department.'\n 'Warning sign re dangers of hookah smoking not posted in each room or area of Non-Tobacco Hookah Establishment (NTHE) where smoking is allowed'\n 'A food containing artificial trans fat, with 0.5 grams or more of trans fat per serving, is being stored, distributed, held for service, used in preparation of a menu item, or served.'\n 'Sign prohibiting smoking or using electronic cigarettes not conspicuously  posted'\n 'Unpasteurized milk or milk product (except certain aged cheese) served.'\n 'Shellfish not from approved source, improperly tagged/labeled; tags not retained for 90 days.'\n '“No Smoking” and/or “Smoking Permitted” sign not conspicuously posted. Health warning not present on “Smoking Permitted” sign.'\n 'Meat, fish or molluscan shellfish served raw or undercooked without prior notification to customer.'\n 'Raw fruit or vegetable not properly washed prior to cutting or serving.'\n 'Eggs found dirty/cracked; liquid, frozen or powdered eggs not pasteurized.'\n 'Sodium Warning - Icon not posted'\n 'Food worker or vendor working or is knowingly or negligently permitted to work in FSE while afflicted with infected wound or reportable communicable disease. No spitting allowed. Spitting anywhere in the establishment is prohibited.'\n 'Duties of an officer of the Department interfered with or obstructed.'\n 'Sodium Warning icon posted on menus, menu boards or food tags for food items that contain 2300mg or more of sodium is not a black and white equilateral triangle; or the  equilateral triangle is not as wide as it is tall, or is not equal in height to the largest letter in the food item’s name, as displayed on the menu, menu board, or tag.'\n 'Letter grade or Grade Pending card not conspicuously posted and visible to passersby.'\n 'Food package (including canned food product or hermetically sealed container) not in good condition to the degree contents are not suitable for human consumption.  Package is swollen, leaking or rusted, without “DO NOT USE” label or not segregated from consumable food items.'\n 'Records and logs not maintained to demonstrate that HACCP plan has been properly implemented.'\n 'A food containing artificial trans-fat, with 0.5 grams or more of trans fat per serving, is being stored, distributed, held for service, used in preparation of a menu item, or served.'\n 'Sodium Warning - Icon not compliant'\n 'Organics mixed with non-organic material'\n 'Notice of the Department of Board of Health mutilated, obstructed, or removed.'\n 'Non-TCS food that has been served to the public being re-served. (Does not apply to wrapped foods where the wrapper seal has not been broken or opened).'\n 'Food not labeled in accordance with the approved HACCP plan'\n 'Failure to post signage in organics collection area.'\n \"Prohibited drink listed on children's meal menu\"\n 'Original label for smoking products sold or offered for sale'\n 'Raw food not properly washed prior to serving.'\n 'Canned food product observed swollen, leaking or rusted, and not segregated from other consumable food items.'\n 'Sign prohibiting entry of persons under 21 years of age not posted at public entryway to Non-Tobacco Hookah Establishment (NTHE)'\n 'Organics containers not provided'\n 'Letter grade or Grade Pending card removed, destroyed, modified, obscured, or otherwise tampered with'\n 'Toilet facility used by women does not have at least one covered garbage receptacle.'\n 'Food not labeled in accordance with HACCP plan.'\n 'Sale of tobacco products, herbal cigarettes, liquid nicotine, shisha, rolling papers or smoking paraphernalia to minors prohibited']\n\n\n\nviolation_list = [\"sewage\",\n                 \"mice\",\n                  \"roaches\",\n                 \"pest\",\n                 \"rodents\",\n                 \"harborage\",\n                 \"vermin\",\n                 \"filth\",\n                 \"flies\",\n                 \"garbage\",\n                 \"receptacle\",\n                 \"organics\",\n                 \"nuisance\",\n                 \"waste\",\n                 \"back-flow\"]\n\n\n# Create regex pattern from complaint_list\nviolation_pattern = \"|\".join(violation_list)  # Combine terms into a regex pattern\n\n# Filter for specific complaint types in 'VIOLATION DESCRIPTION'\nres_inspection_filtered = res_inspection[\n    res_inspection['VIOLATION DESCRIPTION'].str.contains(violation_pattern, case=False, na=False)\n]\n\n\n# Store trimmed data\nres_inspection_filtered.to_csv(\"data/sanitation_related_restaurant_inspections.csv\", index=False)",
    "crumbs": [
      "Data Analysis",
      "Trim Additional Data"
    ]
  },
  {
    "objectID": "analysis/0b-trim-additional-data.html#other-311-requests",
    "href": "analysis/0b-trim-additional-data.html#other-311-requests",
    "title": "Trim Additional Data",
    "section": "3 Other 311 Requests",
    "text": "3 Other 311 Requests\nAdditionally, we extracted complaint records for “derelict vehicles” and “graffiti” from the 311 request parquet. Like trash-related requests, these complaints are also handled by DSNY and reflect poorly managed locations prone to trash dumping. We filtered and stored the records associated with these two complaints separately for further use.\n\n# Load the Master Parquet file\nrequests = pd.read_parquet('data/311_Service_Requests.parquet')\n\n\nfiltered_requests = requests[\n    (requests['Agency'] == 'DSNY')]\n\n\n# Create a column of the complaint creation date\nfiltered_requests = filtered_requests.rename(columns={'Created Date': 'Created Date Time'})\nfiltered_requests = filtered_requests.rename(columns={'Closed Date': 'Closed Date Time'})\n\nfiltered_requests['Created Date'] = pd.to_datetime(filtered_requests['Created Date Time'], format='%m/%d/%Y %I:%M:%S %p').dt.date\n\n# Drop redundant columns\nfiltered_requests = filtered_requests.drop(columns=['Created Date Time', 'Closed Date Time'])\n\nstart_date = datetime.strptime('2022-01-01', '%Y-%m-%d').date()\nend_date = datetime.strptime('2023-12-31', '%Y-%m-%d').date()\n\n\n3.1 Derelict Vehicles\n\nderelict_vehicles = filtered_requests[\n    (filtered_requests['Complaint Type'] == \"Derelict Vehicles\") &\n    (filtered_requests['Created Date'] &gt;= start_date) & \n    (filtered_requests['Created Date'] &lt;= end_date)\n]\n\n\n# Store trimmed data\nderelict_vehicles.to_parquet('data/derelict_vehicles.parquet', engine='pyarrow', compression='snappy', index=False)\n\n\n\n3.2 Graffiti\n\ngraffiti = filtered_requests[\n    (filtered_requests['Complaint Type'] == \"Graffiti\") &\n    (filtered_requests['Created Date'] &gt;= start_date) & \n    (filtered_requests['Created Date'] &lt;= end_date)\n]\n\n\n# Store trimmed data\ngraffiti.to_parquet('data/graffiti.parquet', engine='pyarrow', compression='snappy', index=False)\n\n\n\n3.3 Vacant Lot\nWe considered adding vacant lot as a factor, but there is no data reported under this category for the specified years.",
    "crumbs": [
      "Data Analysis",
      "Trim Additional Data"
    ]
  },
  {
    "objectID": "analysis/index.html",
    "href": "analysis/index.html",
    "title": "Data Analysis",
    "section": "",
    "text": "Data Analysis\nThe data processing and analysis for the project consist of four main parts (seven chapters):\n\nPreprocessing of the large dataset (Chapters 1 and 2), which involves preparing the raw data for subsequent analysis.\nProcessing and visualizing independent and dependent variables for 2022 (used for modeling) and 2023 (used for prediction), as well as spatially joining them with DSNY unit boundaries (Chapters 3 to 5).\nBuilding the model and predicting the sanitation complaint counts for 2024 (Chapter 6).\nProviding optimized trash collection strategies for DSNY based on the prediction results (Chapter 7).\n\nAdditionally, since some of the datasets used in this project are too large to upload to GitHub, you can download the complete datasets using the following link if you wish to rerun the code: https://upenn.box.com/s/p4g3nu7x6uyayhlkaviofbe8f11no3wf.",
    "crumbs": [
      "Data Analysis"
    ]
  },
  {
    "objectID": "analysis/3-gather-and-explore-additional-prediction-data.html",
    "href": "analysis/3-gather-and-explore-additional-prediction-data.html",
    "title": "Gather and Explore Additional Prediction Data",
    "section": "",
    "text": "Similar to the previous step, this section compiles the identical independent features hypothesized to correlate with trash complaint counts for year 2023-24. Note that data from this step will be used for prediction, using best-fit model trained upon 2022-2023 data. Thus the output dataframe at the end of this section lacks the independent variable Complaint Count.",
    "crumbs": [
      "Data Analysis",
      "Gather and Explore Additional Prediction Data"
    ]
  },
  {
    "objectID": "analysis/3-gather-and-explore-additional-prediction-data.html#set-up",
    "href": "analysis/3-gather-and-explore-additional-prediction-data.html#set-up",
    "title": "Gather and Explore Additional Prediction Data",
    "section": "1 Set Up",
    "text": "1 Set Up\n\n\nCode\nimport pandas as pd\nimport geopandas as gpd\nimport numpy as np\nimport osmnx as ox\nimport cenpy\nimport pygris\n\nimport seaborn as sns\n!pip install hvplot\nimport holoviews as hv\nhv.extension('bokeh') \nfrom datashader.colors import viridis\nimport hvplot.pandas\n\nfrom shapely.wkt import loads\nfrom shapely.geometry import MultiLineString\nfrom shapely.ops import nearest_points\n\nfrom datetime import datetime\nimport matplotlib.pyplot as plt\n\n!pip install meteostat\nfrom meteostat import Point, Daily, Stations\n\n!pip install holidays\nimport holidays\n\n# Show all columns in dataframes\npd.options.display.max_columns = 999\n\n# Hide warnings due to issue in shapely package \n# See: https://github.com/shapely/shapely/issues/1345\nnp.seterr(invalid=\"ignore\");",
    "crumbs": [
      "Data Analysis",
      "Gather and Explore Additional Prediction Data"
    ]
  },
  {
    "objectID": "analysis/3-gather-and-explore-additional-prediction-data.html#load-dsny-boundary-and-complaint-data-2022-and-2023",
    "href": "analysis/3-gather-and-explore-additional-prediction-data.html#load-dsny-boundary-and-complaint-data-2022-and-2023",
    "title": "Gather and Explore Additional Prediction Data",
    "section": "2 Load DSNY Boundary and Complaint Data (2022 and 2023)",
    "text": "2 Load DSNY Boundary and Complaint Data (2022 and 2023)\n\n2.1 Load DSNY Boundary\n\n\nCode\nDSNY_boundary = gpd.read_file('data/1_DSNY_boundary_processed.geojson')\n\n# Reproject to state plane\nDSNY_boundary = DSNY_boundary.to_crs(epsg=2263)\n\n\n\n\n2.2 Load Complaint Data\n\n\nCode\n# Load 2023 data\ncomplaints_23 = pd.read_parquet('data/1_complaints_DSNY_full_2023.parquet')\n\n\n\n\nCode\n# Convert date to the right format\ncomplaints_23['Created Date'] = pd.to_datetime(complaints_23['Created Date'], errors='coerce')\n\n\nSince 2024 Complaint Counts are to be predicted rather than existant, a full DSNY unit-date panel is created to hold all forecasted features (count of litter baskets and traffic) for 2024.\nIn addition, because 2024 has 366 days compared to 2023 with 365 days, to ensure all lagged data match, the date 2024-02-29 is removed from the panel.\n\n\nCode\n# Create 2024 full DSNY-date panel\ndate_range = pd.date_range(start=\"2024-01-01\", end=\"2024-12-31\")\n\n# Get unique DSNY units\nunique_IDs = DSNY_boundary['DSNY_ID'].unique()\n\n# Create a dataframe with all DSNY unit-date combinations\nfull_panel = pd.MultiIndex.from_product(\n    [unique_IDs, date_range], names=['DSNY_ID', 'Created Date']\n).to_frame(index=False)\n\ncomplaints_24 = full_panel\n\n\n\n\nCode\n# removing Feb 29th, 2024\ncomplaints_24 = complaints_24[complaints_24['Created Date'] != '2024-02-29']",
    "crumbs": [
      "Data Analysis",
      "Gather and Explore Additional Prediction Data"
    ]
  },
  {
    "objectID": "analysis/3-gather-and-explore-additional-prediction-data.html#lagged-data-gathering",
    "href": "analysis/3-gather-and-explore-additional-prediction-data.html#lagged-data-gathering",
    "title": "Gather and Explore Additional Prediction Data",
    "section": "3 Lagged Data Gathering",
    "text": "3 Lagged Data Gathering\n\n3.1 Gather 2023 ACS 5-year data\n\n3.1.1 Data Gathering\n\n\nCode\nvariables = [\"NAME\",\n             \"B19013_001E\", #median hh income\n             \"B01003_001E\", #total pop\n             \"B03002_003E\", #white alone\n             \"B03002_012E\", #hispanic-latino total\n             \"B06012_002E\", #poverty rate\n             \"B25010_001E\" #average hh size\n            ]\n\nacs = cenpy.remote.APIConnection(\"ACSDT5Y2023\")\n\nNYC_county_code = [\"005\", \"047\", \"061\", \"081\", \"085\"]\nNYC_code = \"36\"\nNYC_county_string = \",\".join(NYC_county_code)\n\nacs_vars = acs.query(\n    cols=variables,\n    geo_unit=\"tract:*\",\n    geo_filter={\"state\": NYC_code, \"county\": NYC_county_string},\n)\n\n# Rename columns\nacs_vars.rename(columns={\n    \"B19013_001E\": \"Med_HH_Inc\", \n    \"B01003_001E\": \"Tot_Pop\", \n    \"B03002_003E\": \"White_Alone\",\n    \"B03002_012E\": \"Hispanic_Latino\",\n    \"B06012_002E\": \"Poverty_Pop\",\n    \"B25010_001E\": \"Avg_HH_Size\"\n}, inplace=True)\n\n# Convert to Numeric\nexclude_cols= ['NAME', 'tract', 'state', 'county']\nacs_vars.loc[:, ~acs_vars.columns.isin(exclude_cols)] = (\n    acs_vars.loc[:, ~acs_vars.columns.isin(exclude_cols)]\n    .apply(pd.to_numeric, errors='coerce')\n)\n\n# Remove invalid results\nacs_vars = acs_vars[\n    (acs_vars['Med_HH_Inc'] != -666666666) & \n    (acs_vars['Avg_HH_Size'] != -666666666)\n]\nacs_vars = acs_vars.reset_index(drop=True)\n\n# calculate race/ethnicity share\nacs_vars['Pct_Non_White'] = (1 - (acs_vars['White_Alone'] / acs_vars['Tot_Pop'])) * 100\nacs_vars['Pct_Hispanic'] = (acs_vars['Hispanic_Latino'] / acs_vars['Tot_Pop']) * 100\nacs_vars['Poverty_Rate'] = (acs_vars['Poverty_Pop'] / acs_vars['Tot_Pop']) * 100\n\n# Modify GEOID of the acs_vars data frame\nacs_vars[\"GEOID\"] = (\n    acs_vars[\"state\"].astype(str).str.zfill(2) +  \n    acs_vars[\"county\"].astype(str).str.zfill(3) +\n    acs_vars[\"tract\"].astype(str).str.zfill(6)\n)\n\n# select necessary columns\nacs_vars = acs_vars[[\"GEOID\", \"Med_HH_Inc\", \"Pct_Non_White\", \"Pct_Hispanic\", \"Poverty_Rate\", \"Avg_HH_Size\"]]\n\n# Get NYC tracts boundaries\nNYC_tracts = pygris.tracts(state=\"36\", county=NYC_county_code, year=2022)\n\n# Merge info with tract boundaries\nacs_vars_tracts= acs_vars.merge(NYC_tracts, on=\"GEOID\", how=\"left\")\n\n# select necessary columns\nacs_vars_tracts = acs_vars_tracts[[\"GEOID\", \"Med_HH_Inc\", \"Pct_Non_White\", \"Pct_Hispanic\", \"Poverty_Rate\", \"Avg_HH_Size\",\"geometry\"]]\n\n\n\n\n3.1.2 Join ACS Variables with DSNY Boundaries\n\n\nCode\n# Calculate centroids of DSNY_boundary\nDSNY_boundary['centroid'] = DSNY_boundary.geometry.centroid\nDSNY_centroids = DSNY_boundary.set_geometry('centroid')\n\nDSNY_boundary = DSNY_boundary.drop(columns=['centroid'])\n\n# Ensure CRS matches between the two datasets\nacs_vars_tracts = gpd.GeoDataFrame(acs_vars_tracts, geometry=acs_vars_tracts['geometry'], crs=\"EPSG:4326\")\nacs_vars_tracts = acs_vars_tracts.to_crs(DSNY_boundary.crs)\n\n# Perform spatial join to match centroids with ACS tract data\nDSNY_boundary_with_acs = gpd.sjoin(DSNY_centroids, acs_vars_tracts, how=\"left\", predicate=\"intersects\").drop(columns=['index_right'])\n\nDSNY_boundary_with_acs = DSNY_boundary_with_acs.drop(columns=['centroid'])\n\n\n\n\n3.1.3 Join ACS Variables with Complaints Data by District\n\n\nCode\ncomplaints_23 = (complaints_23.merge(DSNY_boundary_with_acs, on=\"DSNY_ID\", how=\"left\"))\n\n\n\n\n\n3.2 Gather 2023 NYC Restaurants Violation Data\nNote: the data loaded is pre-processed to filter out only violations related to trash.\nsource: [https://data.cityofnewyork.us/Health/DOHMH-New-York-City-Restaurant-Inspection-Results/43nn-pn8j/about_data]\n\n3.2.1 Data Gathering\n\n\nCode\nrestaurant_violations = pd.read_csv(\"data/sanitation_related_restaurant_inspections.csv\")\n\n\n\n\n3.2.2 Join Restaurant Violations with DSNY Boundaries\n\n\nCode\n# Convert restaurant violations data to a GeoDataFrame\nrestaurant_violations = restaurant_violations[['CAMIS', 'INSPECTION DATE', 'Latitude', 'Longitude']]\n\nrestaurant_violations = gpd.GeoDataFrame(\n  restaurant_violations, geometry=gpd.points_from_xy(restaurant_violations.Longitude, restaurant_violations.Latitude), crs=\"EPSG:4326\"\n  )\n\nrestaurant_violations = restaurant_violations.to_crs(DSNY_boundary.crs)\n\n# Perform spatial join - identify which DSNY district the restaurant is associated with \nDSNY_boundary_with_rv = gpd.sjoin(restaurant_violations, DSNY_boundary, how=\"right\", predicate=\"within\")\n\n# Group violation inspections by district and date\nDSNY_boundary_with_rv = (\n    DSNY_boundary_with_rv.groupby(['DSNY_ID', 'INSPECTION DATE'])['CAMIS']\n    .nunique()\n    .reset_index()\n)\n\nDSNY_boundary_with_rv.rename(columns={'CAMIS': 'Restaurant Inspection Count'}, inplace=True)\n\nDSNY_boundary_with_rv.rename(columns={\"INSPECTION DATE\": \"Created Date\"}, inplace=True)\n\n\n\n\n3.2.3 Join Restaurant Violations with Complaints Data by District and Date\n\n\nCode\n# Join with complaints data 2023\nDSNY_boundary_with_rv['Created Date'] = pd.to_datetime(DSNY_boundary_with_rv['Created Date'], format='%m/%d/%Y', errors='coerce')\n\n# Join with the time series dataframe\ncomplaints_23 = complaints_23.merge(\n    DSNY_boundary_with_rv,\n    on=[\"Created Date\", \"DSNY_ID\"],  \n    how=\"left\"                       \n)\n\n\n\n\n\n3.3 Gather 2023 NYC Other 311 Data\n\n3.3.1 Data Gathering\n\n\nCode\n# Derelict Vehicles\nderelict_vehicles = pd.read_parquet(\"data/derelict_vehicles.parquet\")\n\n# Graffiti\ngraffiti = pd.read_parquet(\"data/graffiti.parquet\")\n\n\n\n\n3.3.2 Join Other 311 Data with DSNY Boundaries\n\n\nCode\n# Join derelict vehicles\n    # Convert derelict vehicles data to a GeoDataFrame\nderelict_vehicles = derelict_vehicles[['Unique Key', 'Created Date', 'Latitude', 'Longitude']]\n\nderelict_vehicles = gpd.GeoDataFrame(\n  derelict_vehicles, geometry=gpd.points_from_xy(derelict_vehicles.Longitude, derelict_vehicles.Latitude), crs=\"EPSG:4326\"\n  )\n\nderelict_vehicles = derelict_vehicles.to_crs(DSNY_boundary.crs)\n\n    # Perform spatial join\nDSNY_boundary_with_dv = gpd.sjoin(derelict_vehicles, DSNY_boundary, how=\"right\", predicate=\"within\")\n\n    # Group derelict vehicles by district and date\nDSNY_boundary_with_dv = (\n    DSNY_boundary_with_dv.groupby(['DSNY_ID', 'Created Date'])['Unique Key']\n    .nunique()\n    .reset_index()\n)\n\nDSNY_boundary_with_dv.rename(columns={'Unique Key': 'Derelict Vehicle Count'}, inplace=True)\n\n\n# Join Graffiti\n    # Convert graffiti data to a GeoDataFrame\ngraffiti = graffiti[['Unique Key', 'Created Date', 'Latitude', 'Longitude']]\n\ngraffiti = gpd.GeoDataFrame(\n  graffiti, geometry=gpd.points_from_xy(graffiti.Longitude, graffiti.Latitude), crs=\"EPSG:4326\"\n  )\n\ngraffiti = graffiti.to_crs(DSNY_boundary.crs)\n\n    # Perform spatial join\nDSNY_boundary_with_g = gpd.sjoin(graffiti, DSNY_boundary, how=\"right\", predicate=\"within\")\n\n    # Group graffiti by district and date\nDSNY_boundary_with_g = (\n    DSNY_boundary_with_g.groupby(['DSNY_ID', 'Created Date'])['Unique Key']\n    .nunique()\n    .reset_index()\n)\n\nDSNY_boundary_with_g.rename(columns={'Unique Key': 'Graffiti Count'}, inplace=True)\n\n\n\n\n3.3.3 Join Other 311 Data with Complaints Data by District and Date\n\n\nCode\n# Make sure dates are in datetime format\nDSNY_boundary_with_dv['Created Date'] = pd.to_datetime(DSNY_boundary_with_dv['Created Date'], errors='coerce')\nDSNY_boundary_with_g['Created Date'] = pd.to_datetime(DSNY_boundary_with_g['Created Date'], errors='coerce')\n\n# Join derelict vehicle records with the time series dataframe\ncomplaints_23 = complaints_23.merge(\n    DSNY_boundary_with_dv,\n    on=[\"Created Date\", \"DSNY_ID\"],  \n    how=\"left\"                       \n)\n\n# Join graffiti records with the time series dataframe\ncomplaints_23 = complaints_23.merge(\n    DSNY_boundary_with_g,\n    on=[\"Created Date\", \"DSNY_ID\"],  \n    how=\"left\"                       \n)\n\n\n\n\n\n3.4 Gather 2023 Weather Data\n\n3.4.1 Data Gathering\n\n\nCode\n# Define time range for weather data\nweather_start = datetime(2023, 1, 1)\nweather_end = datetime(2023, 12, 31)\n\n# Location weather data station\nstations = Stations()\nstations = stations.nearby(40.7128, -74.0060)\nstation = stations.fetch()\n\n\n\n\nCode\n# Location weather data station\nstations = Stations()\nstations = stations.nearby(40.7128, -74.0060)\nstation = stations.fetch()\n\n# Pick station\nJFK = station[station['name'] == \"John F. Kennedy Airport\"]\n\n# Gather data\nweather_data = Daily(JFK, weather_start, weather_end)\nweather_data = weather_data.fetch()\n\n# Reset the index to make 'time' a regular column\nweather_data = weather_data.reset_index()\n\n# Convert 'time' to datetime format\nweather_data['time'] = pd.to_datetime(weather_data['time'], errors='coerce')\n\n# Trim temperature, precipitation and snow data\nweather_cols = ['time', 'tavg', 'prcp', 'snow']\nweather_data = weather_data.loc[:, weather_data.columns.isin(weather_cols)]\n\n# Rename columns\nweather_data.rename(columns={\n    \"tavg\": \"avg_temp\",\n    \"prcp\": \"precipitation\"\n}, inplace=True)\n\n\n\n\n3.4.2 Join Weather Data with Complaints Data by Date\n\n\nCode\ncomplaints_23 = complaints_23.merge(\n    weather_data,\n    left_on=\"Created Date\",  \n    right_on=\"time\",        \n    how=\"left\"             \n)\n\ncomplaints_23 = complaints_23.drop(columns=['time'])\n\n\n\n\nCode\ncomplaints_23\n\n\n\n\n\n\n\n\n\nDSNY_ID\nCreated Date\nComplaint Count\ngeometry\nGEOID\nMed_HH_Inc\nPct_Non_White\nPct_Hispanic\nPoverty_Rate\nAvg_HH_Size\nRestaurant Inspection Count\nDerelict Vehicle Count\nGraffiti Count\navg_temp\nprecipitation\nsnow\n\n\n\n\n0\nBKN011_A\n2023-01-01\n0.0\nMULTIPOLYGON (((995020.254 205025.197, 995260....\n36047056100\n160810.0\n35.139665\n17.234637\n8.296089\n2.05\nNaN\nNaN\nNaN\n9.8\n0.0\n0.0\n\n\n1\nBKN011_A\n2023-01-02\n0.0\nMULTIPOLYGON (((995020.254 205025.197, 995260....\n36047056100\n160810.0\n35.139665\n17.234637\n8.296089\n2.05\nNaN\n1.0\nNaN\n7.8\n0.5\n0.0\n\n\n2\nBKN011_A\n2023-01-03\n0.0\nMULTIPOLYGON (((995020.254 205025.197, 995260....\n36047056100\n160810.0\n35.139665\n17.234637\n8.296089\n2.05\nNaN\nNaN\nNaN\n9.0\n9.1\n0.0\n\n\n3\nBKN011_A\n2023-01-04\n0.0\nMULTIPOLYGON (((995020.254 205025.197, 995260....\n36047056100\n160810.0\n35.139665\n17.234637\n8.296089\n2.05\nNaN\nNaN\nNaN\n10.6\n0.3\n0.0\n\n\n4\nBKN011_A\n2023-01-05\n0.0\nMULTIPOLYGON (((995020.254 205025.197, 995260....\n36047056100\n160810.0\n35.139665\n17.234637\n8.296089\n2.05\nNaN\n1.0\nNaN\n9.6\n1.3\n0.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n222645\nSI038_E\n2023-12-27\n1.0\nMULTIPOLYGON (((921213.717 128503.472, 921135....\n36085024800\n148914.0\n27.681733\n23.328473\n4.019996\n2.8\nNaN\nNaN\nNaN\n6.2\n4.8\n0.0\n\n\n222646\nSI038_E\n2023-12-28\n0.0\nMULTIPOLYGON (((921213.717 128503.472, 921135....\n36085024800\n148914.0\n27.681733\n23.328473\n4.019996\n2.8\nNaN\nNaN\nNaN\n10.2\n39.4\n0.0\n\n\n222647\nSI038_E\n2023-12-29\n0.0\nMULTIPOLYGON (((921213.717 128503.472, 921135....\n36085024800\n148914.0\n27.681733\n23.328473\n4.019996\n2.8\nNaN\nNaN\nNaN\n9.3\n0.5\nNaN\n\n\n222648\nSI038_E\n2023-12-30\n0.0\nMULTIPOLYGON (((921213.717 128503.472, 921135....\n36085024800\n148914.0\n27.681733\n23.328473\n4.019996\n2.8\nNaN\nNaN\nNaN\n6.6\n0.0\nNaN\n\n\n222649\nSI038_E\n2023-12-31\n0.0\nMULTIPOLYGON (((921213.717 128503.472, 921135....\n36085024800\n148914.0\n27.681733\n23.328473\n4.019996\n2.8\nNaN\nNaN\nNaN\n5.1\n0.0\n0.0\n\n\n\n\n222650 rows × 16 columns",
    "crumbs": [
      "Data Analysis",
      "Gather and Explore Additional Prediction Data"
    ]
  },
  {
    "objectID": "analysis/3-gather-and-explore-additional-prediction-data.html#non-lagged-data-gathering",
    "href": "analysis/3-gather-and-explore-additional-prediction-data.html#non-lagged-data-gathering",
    "title": "Gather and Explore Additional Prediction Data",
    "section": "4 Non-Lagged Data Gathering",
    "text": "4 Non-Lagged Data Gathering\n\n4.1 Gather Litter Basket Data\n\n4.1.1 Data Gathering\n\n\nCode\nlitter_basket = pd.read_csv(\"data/DSNY_Litter_Basket_Inventory.csv\")\n\n\n\n\n4.1.2 Join Litter Basket Data with DSNY Boundaries\n\n\nCode\nlitter_basket['point'] = litter_basket['point'].apply(loads)\n\nlitter_basket_gdf = gpd.GeoDataFrame(\n    litter_basket, \n    geometry=litter_basket['point'],\n    crs=\"EPSG:4326\"\n)\n\nlitter_basket_gdf = litter_basket_gdf.to_crs(DSNY_boundary.crs)\n\nDSNY_boundary_with_baskets = gpd.sjoin(litter_basket_gdf, DSNY_boundary, how=\"right\", predicate=\"within\")\n\n# count number of baskets by DSNY units\nDSNY_boundary_with_baskets = (\n    DSNY_boundary_with_baskets.groupby(['DSNY_ID'])['BASKETID']\n    .nunique()\n    .reset_index()\n)\n\nDSNY_boundary_with_baskets.rename(columns={'BASKETID': 'Basket Count'}, inplace=True)\n\n\n\n\n4.1.3 Join Litter Basket Data with Complaints Data by District\n\n\nCode\ncomplaints_24 = (complaints_24.merge(DSNY_boundary_with_baskets, on=\"DSNY_ID\", how=\"left\"))\n\n\n\n\n\n4.2 Gather Traffic Data\n\n4.2.1 Data Gathering\n\n\nCode\n# Load street network in New York using OpenStreetMap API\nplace_name = \"New York, USA\"\nG = ox.graph_from_place(place_name, network_type=\"drive\")\n\n# Convert street network graph to GeoDataFrame\nedges = ox.graph_to_gdfs(G, nodes=False)\nedges = edges.dropna(subset=['name', 'length', 'geometry'])\n\n# Convert the list of street names to a single string\nedges['name'] = edges['name'].apply(lambda x: ', '.join(x) if isinstance(x, list) else x)\n\n# Merge edge geometry with the same name\nedges_grouped = edges.groupby('name').agg({\n    'geometry': lambda x: MultiLineString([geom for geom in x]),\n    'length': 'sum'\n}).reset_index()\n\n# Convert the edge grouped data to GeoDataFrame\nedges_grouped = gpd.GeoDataFrame(edges_grouped, geometry='geometry')\n\n\n\n\nCode\n# Load traffic count data\ntraffic_count = pd.read_csv(\"./data/Traffic_Volume_Counts.csv\")\n\n# Calculate total traffic count for each segment per count date\nretain_columns = ['ID', 'SegmentID', 'Roadway Name', 'From', 'To', 'Direction', 'Date']\nsum_columns = [col for col in traffic_count.columns if col not in retain_columns]\ntraffic_count['count'] = traffic_count[sum_columns].sum(axis=1)\ntraffic_count = traffic_count.drop(columns=sum_columns)\n\n# calculate the average daily traffic on each road\ntraffic_count_grouped = traffic_count.groupby('Roadway Name').agg({ 'count': 'mean' }).reset_index()\n\n\n\n\nCode\ntraffic_count_grouped['Roadway Name'] = traffic_count_grouped['Roadway Name'].str.lower()\nedges_grouped['name'] = edges_grouped['name'].str.lower()\n\n# merge the average daily traffic volume to street network\ntraffic_merged = pd.merge(\n    traffic_count_grouped,\n    edges_grouped,\n    left_on='Roadway Name',  \n    right_on='name',        \n    how='inner'             \n)\n\ntraffic_merged = traffic_merged.drop(columns=['name'])\n\n\n\n\n4.2.2 Join Traffic Data with DSNY Boundaries\n\n\nCode\n# Convert traffic_merged to a GeoDataFrame object with correct state plane projection\ntraffic_merged = gpd.GeoDataFrame(traffic_merged, geometry='geometry')\ntraffic_merged = traffic_merged.set_crs(\"EPSG:4326\") \ntraffic_merged = traffic_merged.to_crs(DSNY_boundary.crs)\n\n# Perform a spatial join to assign roads to DSNY units\nDSNY_boundary_with_traffic = gpd.sjoin(traffic_merged, DSNY_boundary, how=\"inner\", predicate=\"intersects\")\n\n\n\n\nCode\nDSNY_boundary_with_traffic = DSNY_boundary_with_traffic.groupby('DSNY_ID').agg({ 'count': 'mean' }).reset_index()\n\n\n\n\n4.2.3 Join Traffic Data with Complaints Data by District\n\n\nCode\ncomplaints_24 = (complaints_24.merge(DSNY_boundary_with_traffic, on=\"DSNY_ID\", how=\"left\"))\n\ncomplaints_24.rename(columns={'count': 'Traffic Count'}, inplace=True)\n\n\n\n\n\n4.3 Add a Holiday Column\n\n\nCode\ncomplaints_24['Holiday'] = complaints_24['Created Date'].isin(holidays.US())\n\n\n\n\n4.4 Add a Day of the Week Column\n\n\nCode\ncomplaints_24['Day of Week'] = complaints_24['Created Date'].dt.day_name()\n\n\n\n\n4.5 Add a Week Column\n\n\nCode\ncomplaints_24['Week'] = complaints_24['Created Date'].dt.isocalendar().week\n\n\n\n\nCode\ncomplaints_24\n\n\n\n\n\n\n\n\n\nDSNY_ID\nCreated Date\nBasket Count\nTraffic Count\nHoliday\nDay of Week\nWeek\n\n\n\n\n0\nQE141_E\n2024-01-01\n70\n10146.444444\nFalse\nMonday\n1\n\n\n1\nQE141_E\n2024-01-02\n70\n10146.444444\nFalse\nTuesday\n1\n\n\n2\nQE141_E\n2024-01-03\n70\n10146.444444\nFalse\nWednesday\n1\n\n\n3\nQE141_E\n2024-01-04\n70\n10146.444444\nFalse\nThursday\n1\n\n\n4\nQE141_E\n2024-01-05\n70\n10146.444444\nFalse\nFriday\n1\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n222645\nMN073_A\n2024-12-27\n84\n14356.583059\nFalse\nFriday\n52\n\n\n222646\nMN073_A\n2024-12-28\n84\n14356.583059\nFalse\nSaturday\n52\n\n\n222647\nMN073_A\n2024-12-29\n84\n14356.583059\nFalse\nSunday\n52\n\n\n222648\nMN073_A\n2024-12-30\n84\n14356.583059\nFalse\nMonday\n1\n\n\n222649\nMN073_A\n2024-12-31\n84\n14356.583059\nFalse\nTuesday\n1\n\n\n\n\n222650 rows × 7 columns",
    "crumbs": [
      "Data Analysis",
      "Gather and Explore Additional Prediction Data"
    ]
  },
  {
    "objectID": "analysis/3-gather-and-explore-additional-prediction-data.html#merge-2023-and-2024-complaint-data",
    "href": "analysis/3-gather-and-explore-additional-prediction-data.html#merge-2023-and-2024-complaint-data",
    "title": "Gather and Explore Additional Prediction Data",
    "section": "5 Merge 2023 and 2024 Complaint Data",
    "text": "5 Merge 2023 and 2024 Complaint Data\n\n\nCode\n# Transform 2023 data Created Date to Match Date\ncomplaints_23['Match Date'] = complaints_23['Created Date'] + pd.DateOffset(years=1)\ncomplaints_23 = complaints_23.drop(columns=['Created Date'])\n\n# Rename Count column in 2023 data\ncomplaints_23.rename(columns={\"Complaint Count\": \"Complaint Count Lag\"}, inplace=True)\n\n\n\n\nCode\n# Merge the two dataset \nDSNY_pred_final = complaints_23.merge(\n    complaints_24,\n    left_on=['DSNY_ID', 'Match Date'],\n    right_on=['DSNY_ID', 'Created Date'],\n    how='right'\n)\n\nDSNY_pred_final = DSNY_pred_final.drop(columns=['Match Date', 'GEOID'])\nDSNY_pred_final.rename(columns={'Created Date': 'Date'}, inplace=True)\n\nDSNY_pred_final.head()\n\n\n\n\n\n\n\n\n\nDSNY_ID\nComplaint Count Lag\ngeometry\nMed_HH_Inc\nPct_Non_White\nPct_Hispanic\nPoverty_Rate\nAvg_HH_Size\nRestaurant Inspection Count\nDerelict Vehicle Count\nGraffiti Count\navg_temp\nprecipitation\nsnow\nDate\nBasket Count\nTraffic Count\nHoliday\nDay of Week\nWeek\n\n\n\n\n0\nQE141_E\n0.0\nMULTIPOLYGON (((1027896.113 148511.306, 102775...\n102324.0\n25.280899\n17.953102\n12.066439\n2.42\nNaN\nNaN\nNaN\n9.8\n0.0\n0.0\n2024-01-01\n70\n10146.444444\nFalse\nMonday\n1\n\n\n1\nQE141_E\n0.0\nMULTIPOLYGON (((1027896.113 148511.306, 102775...\n102324.0\n25.280899\n17.953102\n12.066439\n2.42\nNaN\nNaN\nNaN\n7.8\n0.5\n0.0\n2024-01-02\n70\n10146.444444\nFalse\nTuesday\n1\n\n\n2\nQE141_E\n2.0\nMULTIPOLYGON (((1027896.113 148511.306, 102775...\n102324.0\n25.280899\n17.953102\n12.066439\n2.42\nNaN\nNaN\nNaN\n9.0\n9.1\n0.0\n2024-01-03\n70\n10146.444444\nFalse\nWednesday\n1\n\n\n3\nQE141_E\n1.0\nMULTIPOLYGON (((1027896.113 148511.306, 102775...\n102324.0\n25.280899\n17.953102\n12.066439\n2.42\nNaN\nNaN\nNaN\n10.6\n0.3\n0.0\n2024-01-04\n70\n10146.444444\nFalse\nThursday\n1\n\n\n4\nQE141_E\n0.0\nMULTIPOLYGON (((1027896.113 148511.306, 102775...\n102324.0\n25.280899\n17.953102\n12.066439\n2.42\nNaN\nNaN\nNaN\n9.6\n1.3\n0.0\n2024-01-05\n70\n10146.444444\nFalse\nFriday\n1\n\n\n\n\n\n\n\n\n5.1 Create a Mapping Master Dataframe\n\n\nCode\nDSNY_pred_final_grouped = DSNY_pred_final.groupby('DSNY_ID').agg({\n    'Complaint Count Lag': 'sum',         \n    'Pct_Non_White': 'mean',    \n    'Pct_Hispanic': 'mean',  \n    'Poverty_Rate': 'mean',  \n    'Avg_HH_Size': 'mean',  \n    'Restaurant Inspection Count': 'sum',\n    'Derelict Vehicle Count':'sum',\n    'Basket Count': 'mean'      \n}).reset_index()\n\n# Rename columns\nDSNY_pred_final_grouped.rename(columns={\n    \"Complaint Count Lag\": \"2023 Annual Total of Complaint Count\",\n    \"Pct_Non_White\": \"2024 Share of Non-White Population\",\n    \"Pct_Hispanic\": \"2024 Share of Hispanic-Latino Population\",\n    \"Poverty_Rate\": \"2024 Poverty Rate\", \n    \"Avg_HH_Size\": \"2024 Average Household Size\",\n    \"Restaurant Inspection Count\": \"2024 Annual Total of Restaurant Inspection Count\",\n    \"Derelict Vehicle Count\": \"2024 Annual Total of Derelict Vehicle Count\",\n    \"Basket Count\": \"2024 Annual Total of Basket Count\"\n}, inplace=True)\n\n\n\n\nCode\nDSNY_pred_final_grouped = DSNY_pred_final_grouped.merge(DSNY_boundary, on=\"DSNY_ID\", how=\"right\")\nDSNY_pred_final_grouped = gpd.GeoDataFrame(DSNY_pred_final_grouped, geometry=DSNY_pred_final_grouped['geometry'], crs=\"EPSG:4326\")\nDSNY_pred_final_grouped = DSNY_pred_final_grouped.to_crs(DSNY_boundary.crs)\n\nDSNY_pred_final_grouped.head()\n\n\n\n\n\n\n\n\n\nDSNY_ID\n2023 Annual Total of Complaint Count\n2024 Share of Non-White Population\n2024 Share of Hispanic-Latino Population\n2024 Poverty Rate\n2024 Average Household Size\n2024 Annual Total of Restaurant Inspection Count\n2024 Annual Total of Derelict Vehicle Count\n2024 Annual Total of Basket Count\ngeometry\n\n\n\n\n0\nQE141_E\n37.0\n25.280899\n17.953102\n12.066439\n2.42\n4.0\n15.0\n70.0\nMULTIPOLYGON (((1027896.113 148511.306, 102775...\n\n\n1\nBKN051_E\n259.0\n99.270073\n53.860930\n13.868613\n3.52\n4.0\n243.0\n25.0\nMULTIPOLYGON (((1016232.847 184210.713, 101621...\n\n\n2\nQE113_C\n39.0\n79.402586\n13.776193\n25.122604\n2.84\n1.0\n22.0\n9.0\nMULTIPOLYGON (((1041777.312 213690.503, 104179...\n\n\n3\nBKS121_C\n126.0\n92.852871\n13.815789\n33.044258\n3.01\n18.0\n17.0\n27.0\nMULTIPOLYGON (((984731.553 170990.590, 984696....\n\n\n4\nBX111_D\n115.0\n78.082192\n44.166273\n9.258385\n2.67\n37.0\n57.0\n39.0\nMULTIPOLYGON (((1024570.859 250385.309, 102458...\n\n\n\n\n\n\n\n\n\n5.2 Map of the highly-correlated factors\nSimilarly, we repeated the mapping of independent variables as in the previous step.\n\n\nCode\nmetrics = [\"2023 Annual Total of Complaint Count\", \"2024 Share of Non-White Population\", \"2024 Share of Hispanic-Latino Population\",\n           \"2024 Poverty Rate\", \"2024 Average Household Size\", \"2024 Annual Total of Restaurant Inspection Count\", \"2024 Annual Total of Derelict Vehicle Count\", \n           \"2024 Annual Total of Basket Count\"]\n\n\n\n\nCode\nnum_cols = 3\nnum_rows = -(-len(metrics) // num_cols)\n\nfig, axes = plt.subplots(num_rows, num_cols, figsize=(18, 5 * num_rows)) \n\naxes = axes.flatten()\n\n# Plot each metric\nfor i, metric in enumerate(metrics):\n    ax = axes[i]\n    DSNY_pred_final_grouped.plot(\n        column=metric, \n        cmap=\"viridis\", \n        legend=True, \n        ax=ax,\n        legend_kwds={\n            'shrink': 0.5,  \n            'location': 'right', \n            'pad': 0.02,\n            'aspect': 20   \n        }\n    )\n    ax.set_title(f\"{metric} in NYC\", fontsize=15)\n\n# Turn off unused subplots\nfor j in range(len(metrics), len(axes)):\n    fig.delaxes(axes[j])\n\n# Adjust layout\nplt.tight_layout()\nplt.show()",
    "crumbs": [
      "Data Analysis",
      "Gather and Explore Additional Prediction Data"
    ]
  },
  {
    "objectID": "analysis/3-gather-and-explore-additional-prediction-data.html#save-data-for-2024-prediction",
    "href": "analysis/3-gather-and-explore-additional-prediction-data.html#save-data-for-2024-prediction",
    "title": "Gather and Explore Additional Prediction Data",
    "section": "6 Save Data for 2024 Prediction",
    "text": "6 Save Data for 2024 Prediction\n\n\nCode\nDSNY_pred_master = DSNY_pred_final.drop(columns=[\"geometry\"])\n\nDSNY_pred_master.to_parquet('data/4_DSNY_pred_master.parquet', index=False)",
    "crumbs": [
      "Data Analysis",
      "Gather and Explore Additional Prediction Data"
    ]
  },
  {
    "objectID": "analysis/4-model-building-validation-and-prediction.html",
    "href": "analysis/4-model-building-validation-and-prediction.html",
    "title": "Model Building, Validation, and Prediction",
    "section": "",
    "text": "In this section, we developed and tested both a Poisson Regression model and a Random Forest model on the modeling dataset, using 2022/23 variables to predict 2023 complaint counts. The process involved splitting the data into test and training sets, setting up a pipeline for model training, calculating the coefficient of determination (R-squared) using a 5-fold cross-validation, and evaluating Mean Absolute Errors (MAEs) across DSNY unit-date pairs.\nBased on these evaluations, we selected the Poisson Regression model as the better option for predicting complaint count data and applied it to the prediction dataset, using 2023/24 variables to forecast 2024 counts.",
    "crumbs": [
      "Data Analysis",
      "Model Building, Validation, and Prediction"
    ]
  },
  {
    "objectID": "analysis/4-model-building-validation-and-prediction.html#set-up",
    "href": "analysis/4-model-building-validation-and-prediction.html#set-up",
    "title": "Model Building, Validation, and Prediction",
    "section": "1 Set Up",
    "text": "1 Set Up\n\n\nCode\nimport numpy as np\nimport pandas as pd\nimport geopandas as gpd\n\nfrom matplotlib import pyplot as plt\nimport hvplot.pandas\n\nimport requests\n\n# Import the machine learning libraries\nfrom sklearn.linear_model import PoissonRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\n\n## Preprocessing\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import OneHotEncoder\n\n# Set the random seed for reproducibility\nnp.random.seed(3)\n\n# Show all columns\npd.options.display.max_columns = 999",
    "crumbs": [
      "Data Analysis",
      "Model Building, Validation, and Prediction"
    ]
  },
  {
    "objectID": "analysis/4-model-building-validation-and-prediction.html#load-master-data",
    "href": "analysis/4-model-building-validation-and-prediction.html#load-master-data",
    "title": "Model Building, Validation, and Prediction",
    "section": "2 Load Master Data",
    "text": "2 Load Master Data\n\n\nCode\nDSNY_master = pd.read_parquet('data/2_DSNY_master.parquet')\n\n\n\n2.1 Select Interested Variables from the previous step\n\n\nCode\nDSNY_master_filtered = DSNY_master[[\"DSNY_ID\", \"Complaint Count\", \"Complaint Count Lag\", \"Pct_Non_White\", \"Pct_Hispanic\",\n                                  \"Poverty_Rate\", \"Avg_HH_Size\", \"Restaurant Inspection Count\", \"Derelict Vehicle Count\",\n                                  \"avg_temp\", \"Basket Count\", \"Traffic Count\", \"Holiday\", \"Week\"]]\n\nDSNY_master_filtered['Week'] = DSNY_master['Week'].astype(str)\n\n\n\n\n2.2 Additional Data Cleaning: Treating NAs Differently\nFor rows where the ACS (American Community Survey) or temperature fields contain NA, they are directly removed. This is because missing values in these fields do not necessarily indicate the absence of data; rather, they may result from unmeasured or unavailable data. Retaining such rows could introduce bias or inaccuracies, so removing them is a more prudent approach.\n\n\nCode\n# remove rows if ACS or weather columns contain NAs\ncolumns_to_check = ['Pct_Non_White', 'Pct_Hispanic', 'Poverty_Rate', 'Avg_HH_Size', 'avg_temp']\n\nDSNY_master_filtered = DSNY_master_filtered.dropna(subset=columns_to_check)\n\n\nFor other fields (such as complaint, derelict vehicle, inspection, traffic, and basket), missing values (NA) are uniformly replaced with 0. This is because NA in these fields is more likely to indicate the absence of related events rather than missing data. Therefore, replacing NA with 0 to represent “no record” is more logical and appropriate.\n\n\nCode\n# replace NAs with zeros for all other columns\ncolumns_to_replace = ['Complaint Count', 'Complaint Count Lag', 'Restaurant Inspection Count', 'Derelict Vehicle Count', 'Basket Count', 'Traffic Count']\n\nDSNY_master_filtered[columns_to_replace] = DSNY_master_filtered[columns_to_replace].fillna(0)\n\n\n\n\nCode\nDSNY_master_filtered.head()\n\n\n\n\n\n\n\n\n\nDSNY_ID\nComplaint Count\nComplaint Count Lag\nPct_Non_White\nPct_Hispanic\nPoverty_Rate\nAvg_HH_Size\nRestaurant Inspection Count\nDerelict Vehicle Count\navg_temp\nBasket Count\nTraffic Count\nHoliday\nWeek\n\n\n\n\n0\nBKN011_A\n0.0\n0.0\n30.224904\n13.878223\n5.951728\n2.11\n0.0\n0.0\n10.1\n47\n9794.421906\nFalse\n52\n\n\n1\nBKN011_A\n0.0\n0.0\n30.224904\n13.878223\n5.951728\n2.11\n0.0\n0.0\n11.5\n47\n9794.421906\nFalse\n1\n\n\n2\nBKN011_A\n0.0\n1.0\n30.224904\n13.878223\n5.951728\n2.11\n0.0\n0.0\n1.2\n47\n9794.421906\nFalse\n1\n\n\n3\nBKN011_A\n0.0\n0.0\n30.224904\n13.878223\n5.951728\n2.11\n0.0\n0.0\n-3.8\n47\n9794.421906\nFalse\n1\n\n\n4\nBKN011_A\n0.0\n0.0\n30.224904\n13.878223\n5.951728\n2.11\n0.0\n0.0\n2.5\n47\n9794.421906\nFalse\n1\n\n\n\n\n\n\n\nImportant Note:\nNext, we will proceed with model building using the 2023 master dataset. Since the complaint data is count-based, we selected Poisson Regression and Random Forest Regression as the two predictive models. By comparing the Mean Squared Error (MSE) and Mean Absolute Error (MAE) of the predictions from both models, we will choose the model with better performance to make predictions on the 2024 master dataset. In Section 5, we will propose some optimization recommendations for DSNY’s Trash Collection Schedules based on the prediction results.",
    "crumbs": [
      "Data Analysis",
      "Model Building, Validation, and Prediction"
    ]
  },
  {
    "objectID": "analysis/4-model-building-validation-and-prediction.html#model-building-i---poisson-regression",
    "href": "analysis/4-model-building-validation-and-prediction.html#model-building-i---poisson-regression",
    "title": "Model Building, Validation, and Prediction",
    "section": "3 Model Building I - Poisson Regression",
    "text": "3 Model Building I - Poisson Regression\nWe fitted and tested the initial Poisson Regression model on a 70/30 split of training and test datasets, consisting of randomly selected records from the master modeling dataset. The independent variables were chosen based on the correlation plots and spatial maps discussed in the previous section, along with additional variables believed to have significance in the literature (Traffic Count, Holiday, Week, and DSNY_ID, the DSNY unit’s unique identifier). A pipeline was constructed to standardize the numerical variables (including all count features due to their significantly differing ranges), one-hot encode categorical variables, and fit to the dependent variable Complaint Count. The resulting model was then used to make predictions on the test dataset.\n\n3.1 Set Regression Features\n\n\nCode\nX = DSNY_master_filtered[['Complaint Count Lag','Pct_Non_White', 'Pct_Hispanic', 'Poverty_Rate', 'Avg_HH_Size', 'avg_temp', \n        'Restaurant Inspection Count', 'Derelict Vehicle Count', 'Basket Count', 'Traffic Count', 'Holiday', 'Week', \"DSNY_ID\"]]\ny = DSNY_master_filtered['Complaint Count']\n\n\n\n\n3.2 Set up a Pipeline\n\n\nCode\nnum_cols = [\n    \"Complaint Count Lag\",\n    \"Pct_Non_White\",\n    \"Pct_Hispanic\",\n    \"Poverty_Rate\",\n    \"Avg_HH_Size\",\n    \"avg_temp\",\n    \"Restaurant Inspection Count\",\n    \"Derelict Vehicle Count\",\n    \"Basket Count\",\n    \"Traffic Count\"\n]\n\ncat_cols = [\"Holiday\", \"Week\", \"DSNY_ID\"]\n\n# Create transformer\ntransformer = ColumnTransformer(\n    transformers=[\n        (\"num\", StandardScaler(), num_cols),\n        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), cat_cols),\n    ]\n)\n\n# Initialize pipeline\npipe = make_pipeline(\n    transformer, PoissonRegressor(alpha=1e-12, max_iter=1000))\n\n\n\n\n3.3 Split Test and Train Data\n\n\nCode\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=3)\n\n# Save test indices for later extraction\ntest_indices = X_test.index\n\n\n\n\n3.4 Fit Model\n\n\nCode\nmodel_1 = pipe.fit(X_train, y_train)\n\n\n\n\n3.5 Make Predictions\n\n\nCode\ny_pred = pipe.predict(X_test)\n\n\n\n\n3.6 Model Evaluation\n\n3.6.1 Cross Validation on Training Dataset\nUsing cross-validation, the model achieved an average R-squared of 0.12 (standard deviation: 0.011), indicating that approximately 12% of the variation in the dependent variable is explained by the independent variables. Comparing to a baseline model of mean of median, the model’s R-squared of 0.12 indicates better prediction capability, as it explains 12% more of the variance. Nonetheless, we note that the R-squared could likely be improved with the inclusion of additional time-series data or more granular records that better capture the differentiation between DSNY units, such as foot-traffic or land use info.\n\n\nCode\n# Perform 5-fold cross-validation\nscores = cross_val_score(pipe, X_train, y_train, cv=5, scoring='r2')\nprint(\"Cross-validated R^2 scores:\", scores)\nprint(\"Mean R^2:\", scores.mean())\nprint(\"Standard deviation of R^2:\", scores.std())\n\n\nCross-validated R^2 scores: [0.10121009 0.1237754  0.11564138 0.12658687 0.13295318]\nMean R^2: 0.12003338227646802\nStandard deviation of R^2: 0.010931403584294239\n\n\n\n\n3.6.2 Mean Squared / Absolute Error of Predicted Dataset\nThe Poisson Regression model has a Mean Squared Error (MSE) of 0.63 and a Mean Absolute Error (MAE) of 0.50.\n\n\nCode\nmse = mean_squared_error(y_test, y_pred)\nmae = mean_absolute_error(y_test, y_pred)\nprint(f\"Mean Squared Error: {mse}\")\nprint(f\"Mean Absolute Error: {mae}\")\n\n\nMean Squared Error: 0.6345969709762334\nMean Absolute Error: 0.4985336344940524\n\n\n\n\nCode\n# Create a copy of X_test and add the original and predicted values\nmodel1_results = X_test.copy()\nmodel1_results['Original'] = y_test\nmodel1_results['Predicted'] = y_pred\n\n# Calculate the absolute error\nmodel1_results['Absolute_Error'] = abs(model1_results['Predicted'] - model1_results['Original'])\n\n\n\n\n3.6.3 Mapping MAEs: Model 1\nWe further joined the predicted results back to the test dataset to compute the absolute error for each unique DSNY unit-date pair. Then, we calculated the MAE for each DSNY unit and visualized the spatial distribution of MAEs in the following map.\nThe results reveal significant spatial variation in model performance. The model tends to overpredict in areas such as Midtown Manhattan, the Bronx, parts of central Brooklyn, and southern Queens, where urban developments are denser and the original complaint counts are higher. In contrast, lower errors are observed in regions like southern Brooklyn and Staten Island, indicating better alignment between predicted and actual counts.\nThese differences suggest that the model performs better in areas with less variability in complaint patterns, while it struggles to accurately capture the dynamics of denser, high-activity urban zones.\n\n\nCode\n# Import the DSNY boundary data\nDSNY_boundary = gpd.read_file(\"data/1_DSNY_boundary_processed.geojson\")\nDSNY_boundary = DSNY_boundary.to_crs(epsg=2263)\n\n\n\n\nCode\n# Group the model results by DSNY_ID and calculate the mean\nmodel1_map_grouped = model1_results.groupby('DSNY_ID').mean()\nmodel1_map_grouped = model1_map_grouped.reset_index()\n\nmodel1_map_grouped.head()\n\n\nC:\\Users\\ztyuu\\AppData\\Local\\Temp\\ipykernel_37864\\219829265.py:2: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n  model1_map_grouped = model1_results.groupby('DSNY_ID').mean()\n\n\n\n\n\n\n\n\n\nDSNY_ID\nComplaint Count Lag\nPct_Non_White\nPct_Hispanic\nPoverty_Rate\nAvg_HH_Size\navg_temp\nRestaurant Inspection Count\nDerelict Vehicle Count\nBasket Count\nTraffic Count\nHoliday\nOriginal\nPredicted\nAbsolute_Error\n\n\n\n\n0\nBKN011_A\n0.627273\n30.224904\n13.878223\n5.951728\n2.11\n12.866364\n0.154545\n0.181818\n47.0\n9794.421906\n0.0\n0.736364\n0.713784\n0.746283\n\n\n1\nBKN011_B\n0.430894\n38.286713\n20.687646\n11.324786\n2.18\n13.301626\n0.113821\n0.219512\n49.0\n8879.006680\n0.0\n0.658537\n0.727300\n0.678283\n\n\n2\nBKN012_A\n0.642202\n51.956912\n36.140036\n17.414722\n2.19\n12.802752\n0.055046\n0.137615\n14.0\n7402.500962\n0.0\n0.917431\n1.301385\n1.052745\n\n\n3\nBKN012_B\n1.304348\n39.505777\n29.107831\n20.699615\n2.15\n13.250435\n0.339130\n0.165217\n97.0\n8893.689957\n0.0\n1.817391\n1.706644\n1.286355\n\n\n4\nBKN013_A\n0.959184\n84.018415\n24.991779\n33.212759\n2.82\n12.941837\n0.081633\n0.214286\n28.0\n7246.675347\n0.0\n1.081633\n1.084030\n0.830075\n\n\n\n\n\n\n\n\n\nCode\n# Join model1_map_grouped with DSNY_boundary\nmodel1_map_grouped = DSNY_boundary.merge(\n    model1_map_grouped, \n    left_on='DSNY_ID', \n    right_on='DSNY_ID'\n)\n\nmodel1_map_grouped = gpd.GeoDataFrame(model1_map_grouped, geometry='geometry')\n\n\n\n\nCode\n# Create a single plot\nfig, ax = plt.subplots(figsize=(12, 8))\n\n# Plot the map based on Absolute_Error column\nmodel1_map_grouped.plot(\n    column='Absolute_Error',\n    cmap=\"viridis\",\n    legend=True,\n    ax=ax,\n    legend_kwds={\n        'shrink': 0.6,\n        'location': 'right',\n        'pad': 0.02,\n        'aspect': 20\n    }\n)\n\n# Set the title\nax.set_title(\"Mean Absolute Error of DSNY Units - Poisson Regression\", fontsize=18)\n\n# Adjust layout and show the plot\nplt.tight_layout()\nplt.show()",
    "crumbs": [
      "Data Analysis",
      "Model Building, Validation, and Prediction"
    ]
  },
  {
    "objectID": "analysis/4-model-building-validation-and-prediction.html#model-building-ii---random-forest-regression",
    "href": "analysis/4-model-building-validation-and-prediction.html#model-building-ii---random-forest-regression",
    "title": "Model Building, Validation, and Prediction",
    "section": "4 Model Building II - Random Forest Regression",
    "text": "4 Model Building II - Random Forest Regression\n\n4.1 Set up a random forest pipeline\nNext, we fitted and tested a second model - the Random Forest model - on the same 70/30 training and test datasets. We adjusted the pipeline from the previous step and replaced the prediction model to a Random Forest, with parameters n_estimators = 100 and max_depth non-specified. The resulting model was then used to make predictions on the test dataset.\n\n\nCode\nrf_pipeline = make_pipeline(transformer, RandomForestRegressor(n_estimators=100, max_depth=None, n_jobs=-1, random_state=3))\n\n\n\n\n4.2 Fit the model\nUse the 70% training data and 30% testing data split from the previous step.\n\n\nCode\nmodel_2 = rf_pipeline.fit(X_train, y_train)\n\n\n\n\n4.3 Make Predictions\n\n\nCode\ny_rf_pred = rf_pipeline.predict(X_test)\n\n\n\n\n4.4 Model Evaluation\n\n4.4.1 Cross Validation on Training Dataset\nComparing to the Poisson Regression model, the Random Forest model performs significantly worse, as evidenced by an overall R-squared of -0.024. This poor performance is primarily due to one of the five folds obtaining an R-squared of -0.25, while the remaining four folds ranged between 0.008 and 0.055. The underperformance of the Random Forest model may be due to the relatively limited independent features, which might not provide enough diversity for a tree-based ensemble model to effectively partition and generalize the data.\n\n\nCode\n# Use a 5-fold cross-validation\nscores = cross_val_score(rf_pipeline, X_train, y_train, cv=5, scoring='r2')\nprint(\"Cross-validated R^2 scores:\", scores)\nprint(\"Mean R^2:\", scores.mean())\nprint(\"Standard deviation of R^2:\", scores.std())\n\n\nCross-validated R^2 scores: [ 0.05532603  0.04083288  0.00802746  0.02453019 -0.2493868 ]\nMean R^2: -0.024134046619593997\nStandard deviation of R^2: 0.11373300037068071\n\n\n\n\n4.4.2 Mean Squared / Absolute Error of Predicted Dataset\nLikewise, with aN MSE of 0.69 and an MAE OF 0.51, the Random Forest model also underperforms compared to the Poisson Regression model.\n\n\nCode\nrf_mse = mean_squared_error(y_test, y_rf_pred)\nrf_mae = mean_absolute_error(y_test, y_rf_pred)\n\nprint(f\"Random Forest Mean Squared Error: {rf_mse}\")\nprint(f\"Random Forest Mean Absolute Error: {rf_mae}\")\n\n\nRandom Forest Mean Squared Error: 0.6943296050519497\nRandom Forest Mean Absolute Error: 0.5082999457597058\n\n\n\n\n4.4.3 Mapping MAEs: Model 2\nEchoing the process completed for Model 1 (Poisson Regression), we joined the predicted results back to the test dataset to calculate the absolute error for each unique DSNY unit-date pair. We then computed the Mean Absolute Error (MAE) for each DSNY unit and visualized the spatial distribution of MAEs through mapping.\nThe map reveals similar spatial biases in model performance like Model 1, including apparent overprediction in areas of denser urban development and higher actual complaint counts (ie. Midtown Manhattan, the Bronx, parts of central Brooklyn, and southern Queens). Comparatively, lower errors are observed in regions like southern Brooklyn and Staten Island, indicating better match between predicted and actual counts. These similar observations highlight inherent limitations in the predictive features, leading to shared shortcomings across both models.\n\n\nCode\n# Create a copy of X_test and add the original and predicted values\nmodel2_results = X_test.copy()\nmodel2_results['Original'] = y_test\nmodel2_results['Predicted'] = y_rf_pred\n\n# Calculate the absolute error\nmodel2_results['Absolute_Error'] = abs(model2_results['Predicted'] - model2_results['Original'])\n\n\n\n\nCode\n# Group the model results by DSNY_ID and calculate the mean\nmodel2_map_grouped = model2_results.groupby('DSNY_ID').mean()\nmodel2_map_grouped = model2_map_grouped.reset_index()\n\nmodel2_map_grouped.head()\n\n\nC:\\Users\\ztyuu\\AppData\\Local\\Temp\\ipykernel_37864\\2418635244.py:2: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n  model2_map_grouped = model2_results.groupby('DSNY_ID').mean()\n\n\n\n\n\n\n\n\n\nDSNY_ID\nComplaint Count Lag\nPct_Non_White\nPct_Hispanic\nPoverty_Rate\nAvg_HH_Size\navg_temp\nRestaurant Inspection Count\nDerelict Vehicle Count\nBasket Count\nTraffic Count\nHoliday\nOriginal\nPredicted\nAbsolute_Error\n\n\n\n\n0\nBKN011_A\n0.627273\n30.224904\n13.878223\n5.951728\n2.11\n12.866364\n0.154545\n0.181818\n47.0\n9794.421906\n0.0\n0.736364\n0.741036\n0.771545\n\n\n1\nBKN011_B\n0.430894\n38.286713\n20.687646\n11.324786\n2.18\n13.301626\n0.113821\n0.219512\n49.0\n8879.006680\n0.0\n0.658537\n0.755989\n0.735935\n\n\n2\nBKN012_A\n0.642202\n51.956912\n36.140036\n17.414722\n2.19\n12.802752\n0.055046\n0.137615\n14.0\n7402.500962\n0.0\n0.917431\n1.155985\n1.134884\n\n\n3\nBKN012_B\n1.304348\n39.505777\n29.107831\n20.699615\n2.15\n13.250435\n0.339130\n0.165217\n97.0\n8893.689957\n0.0\n1.817391\n1.607022\n1.311935\n\n\n4\nBKN013_A\n0.959184\n84.018415\n24.991779\n33.212759\n2.82\n12.941837\n0.081633\n0.214286\n28.0\n7246.675347\n0.0\n1.081633\n1.117143\n0.948571\n\n\n\n\n\n\n\n\n\nCode\n# Join model2_map_grouped with DSNY_boundary\nmodel2_map_grouped = DSNY_boundary.merge(\n    model2_map_grouped, \n    left_on='DSNY_ID', \n    right_on='DSNY_ID'\n)\n\nmodel2_map_grouped = gpd.GeoDataFrame(model2_map_grouped, geometry='geometry')\n\n\n\n\nCode\n# Create a single plot\nfig, ax = plt.subplots(figsize=(12, 8))\n\n# Plot the map based on Absolute_Error column\nmodel2_map_grouped.plot(\n    column='Absolute_Error',\n    cmap=\"viridis\",\n    legend=True,\n    ax=ax,\n    legend_kwds={\n        'shrink': 0.6,\n        'location': 'right',\n        'pad': 0.02,\n        'aspect': 20\n    }\n)\n\n# Set the title\nax.set_title(\"Mean Absolute Error of DSNY Units - Random Forest Regression\", fontsize=18)\n\n# Adjust layout and show the plot\nplt.tight_layout()\nplt.show()",
    "crumbs": [
      "Data Analysis",
      "Model Building, Validation, and Prediction"
    ]
  },
  {
    "objectID": "analysis/4-model-building-validation-and-prediction.html#predict-2024-results",
    "href": "analysis/4-model-building-validation-and-prediction.html#predict-2024-results",
    "title": "Model Building, Validation, and Prediction",
    "section": "5 Predict 2024 Results",
    "text": "5 Predict 2024 Results\nThrough model evaluation and performance comparison, we found that Poisson Regression exhibited significantly better predictive performance than Random Forest Regression. Consequently, Poisson Regression was chosen as the modeling method to predict the number of sanitation complaints for 2024.\n\n5.1 Load 2024 Data\n\n\nCode\nDSNY_2024_master = pd.read_parquet(\"data/4_DSNY_pred_master.parquet\")\n\n\n\n\n5.2 Process 2024\nFor rows where the ACS (American Community Survey) or temperature fields contain NA, they are directly removed. This is because missing values in these fields do not necessarily indicate the absence of data; rather, they may result from unmeasured or unavailable data. Retaining such rows could introduce bias or inaccuracies, so removing them is a more prudent approach.\n\n\nCode\n# remove rows if ACS or weather columns contain NAs\ncolumns_to_check_new = ['Pct_Non_White', 'Pct_Hispanic', 'Poverty_Rate', 'Avg_HH_Size', 'avg_temp']\n\nDSNY_2024_master_filtered = DSNY_2024_master.dropna(subset=columns_to_check_new)\n\n\nFor other fields (such as complaint, derelict vehicle, inspection, traffic, and basket), missing values (NA) are uniformly replaced with 0. This is because NA in these fields is more likely to indicate the absence of related events rather than missing data. Therefore, replacing NA with 0 to represent “no record” is more logical and appropriate.\n\n\nCode\n# replace NAs with zeros for all other columns\ncolumns_to_replace_new = ['Complaint Count Lag', 'Restaurant Inspection Count', 'Derelict Vehicle Count', 'Basket Count', 'Traffic Count']\n\nDSNY_2024_master_filtered[columns_to_replace_new] = DSNY_2024_master_filtered[columns_to_replace_new].fillna(0)\n\n\n\n\n5.3 Use the Better-Fit Model to Predict 2024 Results\n\n\nCode\nX_new = DSNY_2024_master_filtered[['Complaint Count Lag','Pct_Non_White', 'Pct_Hispanic', 'Poverty_Rate', 'Avg_HH_Size', 'avg_temp', \n        'Restaurant Inspection Count', 'Derelict Vehicle Count', 'Basket Count', 'Traffic Count', 'Holiday', 'Week', \"DSNY_ID\"]]\n\nX_new[\"Week\"] = X_new[\"Week\"].astype(str) \nX_new[\"DSNY_ID\"] = X_new[\"DSNY_ID\"].astype(str)\n\n\n\n\nCode\ny_new = model_1.predict(X_new)\n\n\n\n\n5.4 Save Prediction Results\n\n\nCode\npredicted_results_2024 = DSNY_2024_master_filtered.copy()\npredicted_results_2024['Predicted'] = y_new\n\n\n\n\nCode\npredicted_results_2024.to_parquet('data/predicted_results_2024.parquet', index=False)",
    "crumbs": [
      "Data Analysis",
      "Model Building, Validation, and Prediction"
    ]
  },
  {
    "objectID": "analysis/5-application-and-optimization-recommendation.html",
    "href": "analysis/5-application-and-optimization-recommendation.html",
    "title": "Use 2024 Predictions for DSNY Trash Collection Optimization",
    "section": "",
    "text": "In this section, we use the 2024 predicted sanitation complaints counts to help DSNY develop its trash collection strategy for 2024, aiming to maximize collection efficiency.\nAdditionally, since we standardized the predictors during the Poisson Regression model construction in Section 4, the predicted values represent relative counts rather than absolute complaint numbers.\nWe aggregate the predicted complaint counts at different temporal and spatial scales to uncover patterns:\nWe identify the three days with the highest relative complaint numbers for each DSNY unit as a reference for DSNY’s trash collection schedule. We also create a spatial coverage map for each day of the week, showing the areas that need to be covered. This can be used to make flexible adjustments to collection routes or schedules to maximize collection efficiency.\nSpecial attention should be given to the DSNY units with the highest predicted complaint numbers, where increasing collection frequency or days might be considered. Furthermore, additional predictors could be incorporated to further improve the model’s forecasting performance.",
    "crumbs": [
      "Data Analysis",
      "Use 2024 Predictions for DSNY Trash Collection Optimization"
    ]
  },
  {
    "objectID": "analysis/5-application-and-optimization-recommendation.html#set-up",
    "href": "analysis/5-application-and-optimization-recommendation.html#set-up",
    "title": "Use 2024 Predictions for DSNY Trash Collection Optimization",
    "section": "1 Set Up",
    "text": "1 Set Up\n\n\nCode\nimport pandas as pd\nimport geopandas as gpd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\nimport hvplot.pandas\nimport seaborn as sns",
    "crumbs": [
      "Data Analysis",
      "Use 2024 Predictions for DSNY Trash Collection Optimization"
    ]
  },
  {
    "objectID": "analysis/5-application-and-optimization-recommendation.html#load-prediction-results-for-2024",
    "href": "analysis/5-application-and-optimization-recommendation.html#load-prediction-results-for-2024",
    "title": "Use 2024 Predictions for DSNY Trash Collection Optimization",
    "section": "2 Load Prediction Results for 2024",
    "text": "2 Load Prediction Results for 2024\nHere, we loaded the model prediction results from Section 4 - the daily sanitation complaint counts for each DSNY unit in 2024 - as the foundation for subsequent analyses.\n\n\nCode\n# Load the predicted results for 2024\npredicted_results_2024 = pd.read_parquet('data/predicted_results_2024.parquet')",
    "crumbs": [
      "Data Analysis",
      "Use 2024 Predictions for DSNY Trash Collection Optimization"
    ]
  },
  {
    "objectID": "analysis/5-application-and-optimization-recommendation.html#average-sanitation-complaints-by-day-of-week",
    "href": "analysis/5-application-and-optimization-recommendation.html#average-sanitation-complaints-by-day-of-week",
    "title": "Use 2024 Predictions for DSNY Trash Collection Optimization",
    "section": "3 Average Sanitation Complaints by Day of Week",
    "text": "3 Average Sanitation Complaints by Day of Week\nWe analyzed the average daily sanitation complaint counts across all DSNY units for each day of the week. The results show that the differences between days are not significant, and there is no notable distinction between weekends and weekdays. However, Thursday stands out with relatively higher predicted complaint counts.\n\n\nCode\n# Set the order of Day of Week\nday_order = [\"Sunday\", \"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\"]\npredicted_results_2024[\"Day of Week\"] = pd.Categorical(\n    predicted_results_2024[\"Day of Week\"], \n    categories=day_order, \n    ordered=True\n)\n\n# Recalculate the average values\naverage_predicted_dow = (\n    predicted_results_2024.groupby(\"Day of Week\")[\"Predicted\"].mean().reset_index()\n)\n\n# Plot\nplt.figure(figsize=(8, 6))\nplt.bar(\n    average_predicted_dow[\"Day of Week\"], \n    average_predicted_dow[\"Predicted\"] * 10, \n    width=0.6, \n    alpha=0.8\n)\n\nplt.title(\"Mean Predicted Sanitation Complaints for Day of Week - 2024\", fontsize=14)\nplt.xlabel(\"Day of Week\", fontsize=12)\nplt.ylabel(\"Average Predicted Counts\", fontsize=12)\n\nplt.grid(axis='y', linestyle='--', alpha=0.7)\nplt.tight_layout()\nplt.show()",
    "crumbs": [
      "Data Analysis",
      "Use 2024 Predictions for DSNY Trash Collection Optimization"
    ]
  },
  {
    "objectID": "analysis/5-application-and-optimization-recommendation.html#average-sanitation-complaints-by-dsny-units",
    "href": "analysis/5-application-and-optimization-recommendation.html#average-sanitation-complaints-by-dsny-units",
    "title": "Use 2024 Predictions for DSNY Trash Collection Optimization",
    "section": "4 Average Sanitation Complaints by DSNY Units",
    "text": "4 Average Sanitation Complaints by DSNY Units\nWe calculated the average daily sanitation complaint counts for each DSNY unit over the course of the year and identified the top ten areas with the highest predicted counts as key focus points. Additionally, we created a distribution map of the predicted values across the entire region. The map reveals that areas with higher predicted counts are primarily concentrated in the southern part of Manhattan Island and its surrounding areas.\n\n\nCode\n# Group the predicted results by DSNY_ID\naverage_predicted_unit = (\n    predicted_results_2024.groupby(\"DSNY_ID\")[\"Predicted\"].mean().reset_index()\n)\n\naverage_predicted_unit_sorted = average_predicted_unit.sort_values(\n    by=\"Predicted\", ascending=False\n).reset_index(drop=True)\n\naverage_predicted_unit_sorted.head(n=10)\n\n\n\n\n\n\n\n\n\nDSNY_ID\nPredicted\n\n\n\n\n0\nBKN012_B\n1.750697\n\n\n1\nSI037_C\n1.688011\n\n\n2\nBKS074_E\n1.654605\n\n\n3\nMN043_B\n1.650606\n\n\n4\nMN022_A\n1.475389\n\n\n5\nBX062_A\n1.416731\n\n\n6\nBKN032_A\n1.385093\n\n\n7\nBX031_A\n1.362096\n\n\n8\nBKN012_A\n1.360927\n\n\n9\nBKN051_C\n1.325811\n\n\n\n\n\n\n\n\n\nCode\n# Import the DSNY boundary data\nDSNY_boundary = gpd.read_file(\"data/1_DSNY_boundary_processed.geojson\")\nDSNY_boundary = DSNY_boundary.to_crs(epsg=2263)\n\n# Merge the DSNY boundary data with the average predicted unit data\naverage_predicted_unit_boundary = DSNY_boundary.merge(\n    average_predicted_unit,\n    on=\"DSNY_ID\",\n    how=\"inner\"\n)\n\n\n\n\nCode\n# Create a single plot\nfig, ax = plt.subplots(figsize=(12, 8))\n\n# Plot the map based on Predicted column\naverage_predicted_unit_boundary.plot(\n    column='Predicted',\n    cmap=\"viridis\",\n    legend=True,\n    ax=ax,\n    legend_kwds={\n        'shrink': 0.6,\n        'location': 'right',\n        'pad': 0.02,\n        'aspect': 20\n    }\n)\n\n# Set the title\nax.set_title(\"Mean Predicted Sanitation Complaints of DSNY Units - 2024\", fontsize=18)\n\n# Adjust layout and show the plot\nplt.tight_layout()\nplt.show()",
    "crumbs": [
      "Data Analysis",
      "Use 2024 Predictions for DSNY Trash Collection Optimization"
    ]
  },
  {
    "objectID": "analysis/5-application-and-optimization-recommendation.html#average-sanitation-complaints-by-dsny-units-and-day-of-week",
    "href": "analysis/5-application-and-optimization-recommendation.html#average-sanitation-complaints-by-dsny-units-and-day-of-week",
    "title": "Use 2024 Predictions for DSNY Trash Collection Optimization",
    "section": "5 Average Sanitation Complaints by DSNY Units and Day of Week",
    "text": "5 Average Sanitation Complaints by DSNY Units and Day of Week\n\n5.1 Heatmap of Sanitation Complaints for DSNY Units by Day of Week\nThis heatmap presents a cross-analysis of sanitation complaint counts across different DSNY units and days of the week. It shows that the variation in complaint counts across different days within the same unit is not significant, whereas the differences between units are comparatively more pronounced.\n\n\nCode\n# Group the predicted results by DSNY_ID and day of week\naverage_predicted_dow_unit = (\n    predicted_results_2024.groupby([\"DSNY_ID\", \"Day of Week\"])[\"Predicted\"]\n    .mean()\n    .reset_index()\n)\n\n# Create a pivot table for the average predicted counts by DSNY_ID and day of week\npivot_table = average_predicted_dow_unit.pivot(\"DSNY_ID\", \"Day of Week\", \"Predicted\")\n\n# Plot the pivot table as a heatmap\nplt.figure(figsize=(12, 100))  # Adjust the figure height to show all DSNY_IDs\nsns.heatmap(\n    pivot_table, \n    annot=True, \n    fmt=\".3f\",  # Maintain 3 decimal places\n    cmap=\"YlGnBu\", \n    cbar=False  # Remove the color bar\n)\n\nplt.title(\"Mean Predicted Sanitation Complaints by DSNY_ID and Day of Week - 2024\", fontsize=14)\nplt.xlabel(\"Day of Week\", fontsize=12)\nplt.ylabel(\"DSNY_ID\", fontsize=12)\nplt.tight_layout()\nplt.show()\n\n\n/var/folders/4s/r9g1pcw12mv6gt8hx2lkth3h0000gn/T/ipykernel_5374/4086100161.py:9: FutureWarning: In a future version of pandas all arguments of DataFrame.pivot will be keyword-only.\n  pivot_table = average_predicted_dow_unit.pivot(\"DSNY_ID\", \"Day of Week\", \"Predicted\")\n\n\n\n\n\n\n\n\n\n\n\n5.2 Map of the Top Three Days for Sanitation Complaints by DSNY Units\nBased on DSNY’s current trash collection strategy, we ranked the sanitation complaint counts for each DSNY unit across the days of the week and identified the top three days with the highest counts as the recommended trash collection days for each unit. The analysis shows that most units have the highest complaint counts on Thursdays, while the second and third highest days are more scattered, making it difficult to identify a clear pattern. Therefore, further analysis is conducted in the next step.\n\n\nCode\n# Find the top three days of the week for each DSNY_ID\ntop_three_days = (\n    average_predicted_dow_unit.sort_values(by=[\"DSNY_ID\", \"Predicted\"], ascending=[True, False])\n    .groupby(\"DSNY_ID\")\n    .head(3)\n)\n\n# Top three cleaning days per DSNY_ID\nclean_day_result = (\n    top_three_days.assign(rank=top_three_days.groupby(\"DSNY_ID\").cumcount() + 1)\n    .pivot(index=\"DSNY_ID\", columns=\"rank\", values=\"Day of Week\")\n    .reset_index()\n)\n\nclean_day_result.columns = [\"DSNY_ID\", \"Top Day 1\", \"Top Day 2\", \"Top Day 3\"]\n\n# Merge with DSNY boundaries\nclean_day_result = DSNY_boundary.merge(\n    clean_day_result,\n    on=\"DSNY_ID\",\n    how=\"inner\"\n)\n\n\n\n\nCode\n# List of columns to plot\ntop_days = [\"Top Day 1\", \"Top Day 2\", \"Top Day 3\"]\n\n# Set up the grid for subplots (1 row, 3 columns)\nfig, axes = plt.subplots(1, 3, figsize=(18, 6), constrained_layout=True)\n\n# Iterate through each column and plot\nfor i, col in enumerate(top_days):\n    clean_day_result.plot(\n        column=col,\n        cmap=\"viridis\",\n        legend=True,\n        ax=axes[i]\n    )\n    # Set the title for each subplot\n    axes[i].set_title(f\"{col} for DSNY Units - 2024\", fontsize=14)\n    axes[i].axis(\"off\")  # Remove axes for cleaner visualization\n\n# Show the plot\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n5.3 Map of Cleaning Demand Areas for Each Day of the Week by DSNY Units\nIn this step, we created distribution maps based on the top three days with the highest sanitation complaint counts for each DSNY unit, with each map corresponding to a specific day of the week. In the maps, pink areas represent DSNY units that require trash collection on that particular day.\nThese maps provide a clear visualization of the locations needing trash collection each day. By integrating other factors, DSNY can flexibly adjust spatial and temporal trash collection strategies, helping to optimize routes and schedules for maximum collection efficiency.\n\n\nCode\n# List of all days of the week\ndays_of_week = [\"Sunday\", \"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\"]\n\n# Set up the grid for subplots (3 columns, adjust rows automatically)\nn_cols = 3\nn_rows = (len(days_of_week) + n_cols - 1) // n_cols  # Calculate number of rows\nfig, axes = plt.subplots(n_rows, n_cols, figsize=(15, n_rows * 5))\n\n# Flatten the axes for easy indexing\naxes = axes.flatten()\n\n# Iterate through each day and plot\nfor i, day in enumerate(days_of_week):\n    # Filter the GeoDataFrame to include units with the current day in any of the top days\n    filtered_df = clean_day_result[\n        (clean_day_result[\"Top Day 1\"] == day) |\n        (clean_day_result[\"Top Day 2\"] == day) |\n        (clean_day_result[\"Top Day 3\"] == day)\n    ]\n\n    # Create the plot on the respective subplot\n    # Step 1: Plot fill colors\n    DSNY_boundary.plot(\n        ax=axes[i],\n        color=\"lightgrey\",  # Default fill color\n        edgecolor=\"none\",   # No edge for this layer\n        alpha=0.4\n    )\n\n    filtered_df.plot(\n        ax=axes[i],\n        color=\"#FFC0CB\",  # Highlight selected units with pink\n        edgecolor=\"none\"  # No edge for this layer\n    )\n\n    # Step 2: Plot boundary lines\n    DSNY_boundary.boundary.plot(\n        ax=axes[i],\n        edgecolor=\"grey\",  # Boundary line color\n        linewidth=0.4       # Boundary line thickness\n    )\n\n    # Set title for the subplot\n    axes[i].set_title(f\"{day} as a Top Cleaning Day\", fontsize=14)\n    axes[i].axis(\"off\")  # Remove axes\n\n# Turn off unused axes if any\nfor j in range(len(days_of_week), len(axes)):\n    axes[j].axis(\"off\")\n\n# Adjust layout\nplt.tight_layout()\nplt.show()",
    "crumbs": [
      "Data Analysis",
      "Use 2024 Predictions for DSNY Trash Collection Optimization"
    ]
  },
  {
    "objectID": "analysis/2-gather-and-explore-additional-modeling-data.html",
    "href": "analysis/2-gather-and-explore-additional-modeling-data.html",
    "title": "Gather and Explore Additional Modeling Data",
    "section": "",
    "text": "To support more strategic trash removal scheduling, our proposed tool leverages historical data and forecasted trends to provide insights into next year’s anticipated trash reporting incidents. This section compiles the following additional independent features hypothesized to correlate with trash complaint counts for year 2022-2023, which will be used as modeling data in the later sections.\nIn the following steps, we approached the data gathering process into lagged data versus forcasted data, based on careful consideration of data availability and common release date.",
    "crumbs": [
      "Data Analysis",
      "Gather and Explore Additional Modeling Data"
    ]
  },
  {
    "objectID": "analysis/2-gather-and-explore-additional-modeling-data.html#set-up",
    "href": "analysis/2-gather-and-explore-additional-modeling-data.html#set-up",
    "title": "Gather and Explore Additional Modeling Data",
    "section": "1 Set Up",
    "text": "1 Set Up\n\n\nCode\nimport pandas as pd\nimport geopandas as gpd\nimport numpy as np\nimport osmnx as ox\nimport cenpy\nimport pygris\n\nimport seaborn as sns\n!pip install hvplot\nimport holoviews as hv\nhv.extension('bokeh') \nfrom datashader.colors import viridis\nimport hvplot.pandas\n\nfrom shapely.wkt import loads\nfrom shapely.geometry import MultiLineString\nfrom shapely.ops import nearest_points\n\nfrom datetime import datetime\nimport matplotlib.pyplot as plt\n\n!pip install meteostat\nfrom meteostat import Point, Daily, Stations\n\n!pip install holidays\nimport holidays\n\n# Show all columns in dataframes\npd.options.display.max_columns = 999\n\n# Hide warnings due to issue in shapely package \n# See: https://github.com/shapely/shapely/issues/1345\nnp.seterr(invalid=\"ignore\");",
    "crumbs": [
      "Data Analysis",
      "Gather and Explore Additional Modeling Data"
    ]
  },
  {
    "objectID": "analysis/2-gather-and-explore-additional-modeling-data.html#load-dsny-boundary-and-complaint-data-2022-and-2023",
    "href": "analysis/2-gather-and-explore-additional-modeling-data.html#load-dsny-boundary-and-complaint-data-2022-and-2023",
    "title": "Gather and Explore Additional Modeling Data",
    "section": "2 Load DSNY Boundary and Complaint Data (2022 and 2023)",
    "text": "2 Load DSNY Boundary and Complaint Data (2022 and 2023)\n\n2.1 Load DSNY Boundary\n\n\nCode\nDSNY_boundary = gpd.read_file('data/1_DSNY_boundary_processed.geojson')\n\n# Reproject to state plane\nDSNY_boundary = DSNY_boundary.to_crs(epsg=2263)\n\n\n\n\n2.2 Load Complaint Data\n\n\nCode\n# Load 2022 data\ncomplaints_22 = pd.read_parquet('data/1_complaints_DSNY_full_2022.parquet')\n\n\n\n\nCode\n# Load 2023 data\ncomplaints_23 = pd.read_parquet('data/1_complaints_DSNY_full_2023.parquet')\n\n\n\n\nCode\n# Convert date to the right format\ncomplaints_22['Created Date'] = pd.to_datetime(complaints_22['Created Date'], errors='coerce')\ncomplaints_23['Created Date'] = pd.to_datetime(complaints_23['Created Date'], errors='coerce')",
    "crumbs": [
      "Data Analysis",
      "Gather and Explore Additional Modeling Data"
    ]
  },
  {
    "objectID": "analysis/2-gather-and-explore-additional-modeling-data.html#lagged-data-gathering",
    "href": "analysis/2-gather-and-explore-additional-modeling-data.html#lagged-data-gathering",
    "title": "Gather and Explore Additional Modeling Data",
    "section": "3 Lagged Data Gathering",
    "text": "3 Lagged Data Gathering\nA key restriction to the model’s feasibility is availability of data. To predict an upcoming year’s trash removal taskforce scheduling, we will leverage the following data based on the previous years’ records, including: - sociodemographic factors: median household income, share of non-white population, share of hispanic-latino population, poverty rate, and average household size - restaurant violation inspections - other 311 requests: derelict vehicles and graffiti - weather: daily records of average temperature, precipitation, and snow - lagged counts: ie. Number of Complaints filed on 2022-01-01 will be used to predict for 2023-01-01, completed in Section 1\n\n3.1 Gather 2022 ACS 5-year data\n\n3.1.1 Data Gathering\n\n\nCode\nvariables = [\"NAME\",\n             \"B19013_001E\", #median hh income\n             \"B01003_001E\", #total pop\n             \"B03002_003E\", #white alone\n             \"B03002_012E\", #hispanic-latino total\n             \"B06012_002E\", #poverty rate\n             \"B25010_001E\" #average hh size\n            ]\n\nacs = cenpy.remote.APIConnection(\"ACSDT5Y2022\")\n\nNYC_county_code = [\"005\", \"047\", \"061\", \"081\", \"085\"]\nNYC_code = \"36\"\nNYC_county_string = \",\".join(NYC_county_code)\n\nacs_vars = acs.query(\n    cols=variables,\n    geo_unit=\"tract:*\",\n    geo_filter={\"state\": NYC_code, \"county\": NYC_county_string},\n)\n\n# Rename columns\nacs_vars.rename(columns={\n    \"B19013_001E\": \"Med_HH_Inc\", \n    \"B01003_001E\": \"Tot_Pop\", \n    \"B03002_003E\": \"White_Alone\",\n    \"B03002_012E\": \"Hispanic_Latino\",\n    \"B06012_002E\": \"Poverty_Pop\",\n    \"B25010_001E\": \"Avg_HH_Size\"\n}, inplace=True)\n\n# Convert to Numeric\nexclude_cols= ['NAME', 'tract', 'state', 'county']\nacs_vars.loc[:, ~acs_vars.columns.isin(exclude_cols)] = (\n    acs_vars.loc[:, ~acs_vars.columns.isin(exclude_cols)]\n    .apply(pd.to_numeric, errors='coerce')\n)\n\n# Remove invalid results\nacs_vars = acs_vars[\n    (acs_vars['Med_HH_Inc'] != -666666666) & \n    (acs_vars['Avg_HH_Size'] != -666666666)\n]\nacs_vars = acs_vars.reset_index(drop=True)\n\n# calculate race/ethnicity share\nacs_vars['Pct_Non_White'] = (1 - (acs_vars['White_Alone'] / acs_vars['Tot_Pop'])) * 100\nacs_vars['Pct_Hispanic'] = (acs_vars['Hispanic_Latino'] / acs_vars['Tot_Pop']) * 100\nacs_vars['Poverty_Rate'] = (acs_vars['Poverty_Pop'] / acs_vars['Tot_Pop']) * 100\n\n# Modify GEOID of the acs_vars data frame\nacs_vars[\"GEOID\"] = (\n    acs_vars[\"state\"].astype(str).str.zfill(2) +  \n    acs_vars[\"county\"].astype(str).str.zfill(3) +\n    acs_vars[\"tract\"].astype(str).str.zfill(6)\n)\n\n# select necessary columns\nacs_vars = acs_vars[[\"GEOID\", \"Med_HH_Inc\", \"Pct_Non_White\", \"Pct_Hispanic\", \"Poverty_Rate\", \"Avg_HH_Size\"]]\n\n# Get NYC tracts boundaries\nNYC_tracts = pygris.tracts(state=\"36\", county=NYC_county_code, year=2022)\n\n# Merge info with tract boundaries\nacs_vars_tracts= acs_vars.merge(NYC_tracts, on=\"GEOID\", how=\"left\")\n\n# select necessary columns\nacs_vars_tracts = acs_vars_tracts[[\"GEOID\", \"Med_HH_Inc\", \"Pct_Non_White\", \"Pct_Hispanic\", \"Poverty_Rate\", \"Avg_HH_Size\",\"geometry\"]]\n\n\n\n\n3.1.2 Join ACS Variables with DSNY Boundaries\n\n\nCode\n# Calculate centroids of DSNY_boundary\nDSNY_boundary['centroid'] = DSNY_boundary.geometry.centroid\nDSNY_centroids = DSNY_boundary.set_geometry('centroid')\n\nDSNY_boundary = DSNY_boundary.drop(columns=['centroid'])\n\n# Ensure CRS matches between the two datasets\nacs_vars_tracts = gpd.GeoDataFrame(acs_vars_tracts, geometry=acs_vars_tracts['geometry'], crs=\"EPSG:4326\")\nacs_vars_tracts = acs_vars_tracts.to_crs(DSNY_boundary.crs)\n\n# Perform spatial join to match centroids with ACS tract data\nDSNY_boundary_with_acs = gpd.sjoin(DSNY_centroids, acs_vars_tracts, how=\"left\", predicate=\"intersects\").drop(columns=['index_right'])\n\nDSNY_boundary_with_acs = DSNY_boundary_with_acs.drop(columns=['centroid'])\n\n\n\n\n3.1.3 Join ACS Variables with Complaints Data by District\n\n\nCode\ncomplaints_22 = (complaints_22.merge(DSNY_boundary_with_acs, on=\"DSNY_ID\", how=\"left\"))\n\n\n\n\n\n3.2 Gather 2022 NYC Restaurants Violation Data\nNote: the data loaded is pre-processed in section “Trim Additional Data”.\n\n3.2.1 Data Gathering\n\n\nCode\nrestaurant_violations = pd.read_csv(\"data/sanitation_related_restaurant_inspections.csv\")\n\n\n\n\n3.2.2 Join Restaurant Violations with DSNY Boundaries\n\n\nCode\n# Convert restaurant violations data to a GeoDataFrame\nrestaurant_violations = restaurant_violations[['CAMIS', 'INSPECTION DATE', 'Latitude', 'Longitude']]\n\nrestaurant_violations = gpd.GeoDataFrame(\n  restaurant_violations, geometry=gpd.points_from_xy(restaurant_violations.Longitude, restaurant_violations.Latitude), crs=\"EPSG:4326\"\n  )\n\nrestaurant_violations = restaurant_violations.to_crs(DSNY_boundary.crs)\n\n# Perform spatial join - identify which DSNY district the restaurant is associated with \nDSNY_boundary_with_rv = gpd.sjoin(restaurant_violations, DSNY_boundary, how=\"right\", predicate=\"within\")\n\n# Group violation inspections by district and date\nDSNY_boundary_with_rv = (\n    DSNY_boundary_with_rv.groupby(['DSNY_ID', 'INSPECTION DATE'])['CAMIS']\n    .nunique()\n    .reset_index()\n)\n\nDSNY_boundary_with_rv.rename(columns={'CAMIS': 'Restaurant Inspection Count'}, inplace=True)\n\nDSNY_boundary_with_rv.rename(columns={\"INSPECTION DATE\": \"Created Date\"}, inplace=True)\n\n\n\n\n3.2.3 Join Restaurant Violations with Complaints Data by District and Date\n\n\nCode\n# Join with complaints data 2022\nDSNY_boundary_with_rv['Created Date'] = pd.to_datetime(DSNY_boundary_with_rv['Created Date'], format='%m/%d/%Y', errors='coerce')\n\n# Join with the time series dataframe\ncomplaints_22 = complaints_22.merge(\n    DSNY_boundary_with_rv,\n    on=[\"Created Date\", \"DSNY_ID\"],  \n    how=\"left\"                       \n)\n\n\n\n\n\n3.3 Gather 2022 NYC Other 311 Data\nNote: The loaded datasets were pre-processed in section “Trim Additional Data”.\n\n3.3.1 Data Gathering\n\n\nCode\n# Derelict Vehicles\nderelict_vehicles = pd.read_parquet(\"data/derelict_vehicles.parquet\")\n\n# Graffiti\ngraffiti = pd.read_parquet(\"data/graffiti.parquet\")\n\n\n\n\n3.3.2 Join Other 311 Data with DSNY Boundaries\n\n\nCode\n# Join derelict vehicles\n    # Convert derelict vehicles data to a GeoDataFrame\nderelict_vehicles = derelict_vehicles[['Unique Key', 'Created Date', 'Latitude', 'Longitude']]\n\nderelict_vehicles = gpd.GeoDataFrame(\n  derelict_vehicles, geometry=gpd.points_from_xy(derelict_vehicles.Longitude, derelict_vehicles.Latitude), crs=\"EPSG:4326\"\n  )\n\nderelict_vehicles = derelict_vehicles.to_crs(DSNY_boundary.crs)\n\n    # Perform spatial join\nDSNY_boundary_with_dv = gpd.sjoin(derelict_vehicles, DSNY_boundary, how=\"right\", predicate=\"within\")\n\n    # Group derelict vehicles by district and date\nDSNY_boundary_with_dv = (\n    DSNY_boundary_with_dv.groupby(['DSNY_ID', 'Created Date'])['Unique Key']\n    .nunique()\n    .reset_index()\n)\n\nDSNY_boundary_with_dv.rename(columns={'Unique Key': 'Derelict Vehicle Count'}, inplace=True)\n\n\n# Join Graffiti\n    # Convert graffiti data to a GeoDataFrame\ngraffiti = graffiti[['Unique Key', 'Created Date', 'Latitude', 'Longitude']]\n\ngraffiti = gpd.GeoDataFrame(\n  graffiti, geometry=gpd.points_from_xy(graffiti.Longitude, graffiti.Latitude), crs=\"EPSG:4326\"\n  )\n\ngraffiti = graffiti.to_crs(DSNY_boundary.crs)\n\n    # Perform spatial join\nDSNY_boundary_with_g = gpd.sjoin(graffiti, DSNY_boundary, how=\"right\", predicate=\"within\")\n\n    # Group graffiti by district and date\nDSNY_boundary_with_g = (\n    DSNY_boundary_with_g.groupby(['DSNY_ID', 'Created Date'])['Unique Key']\n    .nunique()\n    .reset_index()\n)\n\nDSNY_boundary_with_g.rename(columns={'Unique Key': 'Graffiti Count'}, inplace=True)\n\n\n\n\n3.3.3 Join Other 311 Data with Complaints Data by District and Date\n\n\nCode\n# Make sure dates are in datetime format\nDSNY_boundary_with_dv['Created Date'] = pd.to_datetime(DSNY_boundary_with_dv['Created Date'], errors='coerce')\nDSNY_boundary_with_g['Created Date'] = pd.to_datetime(DSNY_boundary_with_g['Created Date'], errors='coerce')\n\n# Join derelict vehicle records with the time series dataframe\ncomplaints_22 = complaints_22.merge(\n    DSNY_boundary_with_dv,\n    on=[\"Created Date\", \"DSNY_ID\"],  \n    how=\"left\"                       \n)\n\n# Join graffiti records with the time series dataframe\ncomplaints_22 = complaints_22.merge(\n    DSNY_boundary_with_g,\n    on=[\"Created Date\", \"DSNY_ID\"],  \n    how=\"left\"                       \n)\n\n\n\n\n\n3.4 Gather 2022 Weather Data\n\n3.4.1 Data Gathering\n\n\nCode\n# Define time range for weather data\nweather_start = datetime(2022, 1, 1)\nweather_end = datetime(2022, 12, 31)\n\n# Location weather data station\nstations = Stations()\nstations = stations.nearby(40.7128, -74.0060)\nstation = stations.fetch()\n\n\n\n\nCode\n# Location weather data station\nstations = Stations()\nstations = stations.nearby(40.7128, -74.0060)\nstation = stations.fetch()\n\n# Pick station\nJFK = station[station['name'] == \"John F. Kennedy Airport\"]\n\n# Gather data\nweather_data = Daily(JFK, weather_start, weather_end)\nweather_data = weather_data.fetch()\n\n# Reset the index to make 'time' a regular column\nweather_data = weather_data.reset_index()\n\n# Convert 'time' to datetime format\nweather_data['time'] = pd.to_datetime(weather_data['time'], errors='coerce')\n\n# Trim temperature, precipitation and snow data\nweather_cols = ['time', 'tavg', 'prcp', 'snow']\nweather_data = weather_data.loc[:, weather_data.columns.isin(weather_cols)]\n\n# Rename columns\nweather_data.rename(columns={\n    \"tavg\": \"avg_temp\",\n    \"prcp\": \"precipitation\"\n}, inplace=True)\n\n\n\n\n3.4.2 Join Weather Data with Complaints Data by Date\n\n\nCode\ncomplaints_22 = complaints_22.merge(\n    weather_data,\n    left_on=\"Created Date\",  \n    right_on=\"time\",        \n    how=\"left\"             \n)\n\ncomplaints_22 = complaints_22.drop(columns=['time'])\n\n\n\n\nCode\ncomplaints_22\n\n\n\n\n\n\n\n\n\nDSNY_ID\nCreated Date\nComplaint Count\ngeometry\nGEOID\nMed_HH_Inc\nPct_Non_White\nPct_Hispanic\nPoverty_Rate\nAvg_HH_Size\nRestaurant Inspection Count\nDerelict Vehicle Count\nGraffiti Count\navg_temp\nprecipitation\nsnow\n\n\n\n\n0\nBKN011_A\n2022-01-01\n0.0\nMULTIPOLYGON (((995020.254 205025.197, 995260....\n36047056100\n146250.0\n30.224904\n13.878223\n5.951728\n2.11\nNaN\nNaN\nNaN\n10.1\n31.0\n0.0\n\n\n1\nBKN011_A\n2022-01-02\n0.0\nMULTIPOLYGON (((995020.254 205025.197, 995260....\n36047056100\n146250.0\n30.224904\n13.878223\n5.951728\n2.11\nNaN\nNaN\nNaN\n11.5\n0.8\n0.0\n\n\n2\nBKN011_A\n2022-01-03\n1.0\nMULTIPOLYGON (((995020.254 205025.197, 995260....\n36047056100\n146250.0\n30.224904\n13.878223\n5.951728\n2.11\nNaN\nNaN\nNaN\n1.2\n0.0\n0.0\n\n\n3\nBKN011_A\n2022-01-04\n0.0\nMULTIPOLYGON (((995020.254 205025.197, 995260....\n36047056100\n146250.0\n30.224904\n13.878223\n5.951728\n2.11\nNaN\nNaN\nNaN\n-3.8\n0.0\n0.0\n\n\n4\nBKN011_A\n2022-01-05\n0.0\nMULTIPOLYGON (((995020.254 205025.197, 995260....\n36047056100\n146250.0\n30.224904\n13.878223\n5.951728\n2.11\nNaN\nNaN\nNaN\n2.5\n7.4\n0.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n222645\nSI038_E\n2022-12-27\n1.0\nMULTIPOLYGON (((921213.717 128503.472, 921135....\n36085024800\n131146.0\n19.098458\n16.152629\n6.58363\n2.94\nNaN\nNaN\nNaN\n-0.9\n0.0\n0.0\n\n\n222646\nSI038_E\n2022-12-28\n0.0\nMULTIPOLYGON (((921213.717 128503.472, 921135....\n36085024800\n131146.0\n19.098458\n16.152629\n6.58363\n2.94\nNaN\nNaN\nNaN\n2.1\n0.0\n0.0\n\n\n222647\nSI038_E\n2022-12-29\n0.0\nMULTIPOLYGON (((921213.717 128503.472, 921135....\n36085024800\n131146.0\n19.098458\n16.152629\n6.58363\n2.94\nNaN\nNaN\nNaN\n3.7\n0.0\n0.0\n\n\n222648\nSI038_E\n2022-12-30\n0.0\nMULTIPOLYGON (((921213.717 128503.472, 921135....\n36085024800\n131146.0\n19.098458\n16.152629\n6.58363\n2.94\nNaN\nNaN\nNaN\n5.4\n0.0\n0.0\n\n\n222649\nSI038_E\n2022-12-31\n0.0\nMULTIPOLYGON (((921213.717 128503.472, 921135....\n36085024800\n131146.0\n19.098458\n16.152629\n6.58363\n2.94\nNaN\nNaN\nNaN\n7.9\n6.9\n0.0\n\n\n\n\n222650 rows × 16 columns",
    "crumbs": [
      "Data Analysis",
      "Gather and Explore Additional Modeling Data"
    ]
  },
  {
    "objectID": "analysis/2-gather-and-explore-additional-modeling-data.html#forecasted-non-lagged-data-gathering",
    "href": "analysis/2-gather-and-explore-additional-modeling-data.html#forecasted-non-lagged-data-gathering",
    "title": "Gather and Explore Additional Modeling Data",
    "section": "4 Forecasted (Non-Lagged) Data Gathering",
    "text": "4 Forecasted (Non-Lagged) Data Gathering\nThe following steps gather and process forecasted variables based on historic records. Since spatial factors (e.g., location of litter baskets, wider vs. narrower roads) tend to remain relatively constant over time, we projected counts and averages from past year(s) as forecasts for 2023 (the prediction year).\n\nlitter basket: the distribution of litter basket likely have an impact on accessibility of waste disposal locations, hence impacting local sanitation conditions.\ntraffic (auto): the aggregated volume of road traffic in a neighborhood is an indicator of human activities and business vibrancy, thereby influencing the chance of trashing.\n\nsource: - DSNY Litter Basket, NYC Open Data [https://data.cityofnewyork.us/dataset/DSNY-Litter-Basket-Map-/d6m8-cwh9] - Traffic Volume Counts, NYC Open Data [https://data.cityofnewyork.us/Transportation/Traffic-Volume-Counts/btm5-ppia/about_data]\n\n4.1 Gather Litter Basket Data\n\n4.1.1 Data Gathering\n\n\nCode\nlitter_basket = pd.read_csv(\"data/DSNY_Litter_Basket_Inventory.csv\")\n\n\n\n\n4.1.2 Join Litter Basket Data with DSNY Boundaries\n\n\nCode\nlitter_basket['point'] = litter_basket['point'].apply(loads)\n\nlitter_basket_gdf = gpd.GeoDataFrame(\n    litter_basket, \n    geometry=litter_basket['point'],\n    crs=\"EPSG:4326\"\n)\n\nlitter_basket_gdf = litter_basket_gdf.to_crs(DSNY_boundary.crs)\n\nDSNY_boundary_with_baskets = gpd.sjoin(litter_basket_gdf, DSNY_boundary, how=\"right\", predicate=\"within\")\n\n# count number of baskets by DSNY units\nDSNY_boundary_with_baskets = (\n    DSNY_boundary_with_baskets.groupby(['DSNY_ID'])['BASKETID']\n    .nunique()\n    .reset_index()\n)\n\nDSNY_boundary_with_baskets.rename(columns={'BASKETID': 'Basket Count'}, inplace=True)\n\n\n\n\n4.1.3 Join Litter Basket Data with Complaints Data by District\n\n\nCode\ncomplaints_23 = (complaints_23.merge(DSNY_boundary_with_baskets, on=\"DSNY_ID\", how=\"left\"))\n\n\n\n\n\n4.2 Gather Traffic Data\n\n4.2.1 Data Gathering\n\n\nCode\n# Load street network in New York using OpenStreetMap API\nplace_name = \"New York, USA\"\nG = ox.graph_from_place(place_name, network_type=\"drive\")\n\n# Convert street network graph to GeoDataFrame\nedges = ox.graph_to_gdfs(G, nodes=False)\nedges = edges.dropna(subset=['name', 'length', 'geometry'])\n\n# Convert the list of street names to a single string\nedges['name'] = edges['name'].apply(lambda x: ', '.join(x) if isinstance(x, list) else x)\n\n# Merge edge geometry with the same name\nedges_grouped = edges.groupby('name').agg({\n    'geometry': lambda x: MultiLineString([geom for geom in x]),\n    'length': 'sum'\n}).reset_index()\n\n# Convert the edge grouped data to GeoDataFrame\nedges_grouped = gpd.GeoDataFrame(edges_grouped, geometry='geometry')\n\n\n\n\nCode\n# Load traffic count data\ntraffic_count = pd.read_csv(\"./data/Traffic_Volume_Counts.csv\")\n\n# Calculate total traffic count for each segment per count date\nretain_columns = ['ID', 'SegmentID', 'Roadway Name', 'From', 'To', 'Direction', 'Date']\nsum_columns = [col for col in traffic_count.columns if col not in retain_columns]\ntraffic_count['count'] = traffic_count[sum_columns].sum(axis=1)\ntraffic_count = traffic_count.drop(columns=sum_columns)\n\n# calculate the average daily traffic on each road\ntraffic_count_grouped = traffic_count.groupby('Roadway Name').agg({ 'count': 'mean' }).reset_index()\n\n\n\n\nCode\ntraffic_count_grouped['Roadway Name'] = traffic_count_grouped['Roadway Name'].str.lower()\nedges_grouped['name'] = edges_grouped['name'].str.lower()\n\n# merge the average daily traffic volume to street network\ntraffic_merged = pd.merge(\n    traffic_count_grouped,\n    edges_grouped,\n    left_on='Roadway Name',  \n    right_on='name',        \n    how='inner'             \n)\n\ntraffic_merged = traffic_merged.drop(columns=['name'])\n\n\n\n\n4.2.2 Join Traffic Data with DSNY Boundaries\n\n\nCode\n# Convert traffic_merged to a GeoDataFrame object with correct state plane projection\ntraffic_merged = gpd.GeoDataFrame(traffic_merged, geometry='geometry')\ntraffic_merged = traffic_merged.set_crs(\"EPSG:4326\") \ntraffic_merged = traffic_merged.to_crs(DSNY_boundary.crs)\n\n# Perform a spatial join to assign roads to DSNY units\nDSNY_boundary_with_traffic = gpd.sjoin(traffic_merged, DSNY_boundary, how=\"inner\", predicate=\"intersects\")\n\n\n\n\nCode\nDSNY_boundary_with_traffic = DSNY_boundary_with_traffic.groupby('DSNY_ID').agg({ 'count': 'mean' }).reset_index()\n\n\n\n\n4.2.3 Join Traffic Data with Complaints Data by District\n\n\nCode\ncomplaints_23 = (complaints_23.merge(DSNY_boundary_with_traffic, on=\"DSNY_ID\", how=\"left\"))\n\ncomplaints_23.rename(columns={'count': 'Traffic Count'}, inplace=True)\n\n\n\n\n\n4.3 Add a Holiday Column\n\n\nCode\ncomplaints_23['Holiday'] = complaints_23['Created Date'].isin(holidays.US())\n\n\n\n\n4.4 Add a Day of the Week Column\n\n\nCode\ncomplaints_23['Day of Week'] = complaints_23['Created Date'].dt.day_name()\n\n\n\n\n4.5 Add a Week Column\n\n\nCode\ncomplaints_23['Week'] = complaints_23['Created Date'].dt.isocalendar().week\n\n\n\n\nCode\ncomplaints_23\n\n\n\n\n\n\n\n\n\nDSNY_ID\nCreated Date\nComplaint Count\nBasket Count\nTraffic Count\nHoliday\nDay of Week\nWeek\n\n\n\n\n0\nBKN011_A\n2023-01-01\n0.0\n47\n9794.421906\nFalse\nSunday\n52\n\n\n1\nBKN011_A\n2023-01-02\n0.0\n47\n9794.421906\nFalse\nMonday\n1\n\n\n2\nBKN011_A\n2023-01-03\n0.0\n47\n9794.421906\nFalse\nTuesday\n1\n\n\n3\nBKN011_A\n2023-01-04\n0.0\n47\n9794.421906\nFalse\nWednesday\n1\n\n\n4\nBKN011_A\n2023-01-05\n0.0\n47\n9794.421906\nFalse\nThursday\n1\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n222645\nSI038_E\n2023-12-27\n1.0\n6\n13195.713605\nFalse\nWednesday\n52\n\n\n222646\nSI038_E\n2023-12-28\n0.0\n6\n13195.713605\nFalse\nThursday\n52\n\n\n222647\nSI038_E\n2023-12-29\n0.0\n6\n13195.713605\nFalse\nFriday\n52\n\n\n222648\nSI038_E\n2023-12-30\n0.0\n6\n13195.713605\nFalse\nSaturday\n52\n\n\n222649\nSI038_E\n2023-12-31\n0.0\n6\n13195.713605\nFalse\nSunday\n52\n\n\n\n\n222650 rows × 8 columns",
    "crumbs": [
      "Data Analysis",
      "Gather and Explore Additional Modeling Data"
    ]
  },
  {
    "objectID": "analysis/2-gather-and-explore-additional-modeling-data.html#merge-lagged-and-forecasted-variables-data",
    "href": "analysis/2-gather-and-explore-additional-modeling-data.html#merge-lagged-and-forecasted-variables-data",
    "title": "Gather and Explore Additional Modeling Data",
    "section": "5 Merge Lagged and Forecasted Variables Data",
    "text": "5 Merge Lagged and Forecasted Variables Data\nAll 2022 variables are advanced by one year to align with the exact same dates in the 2023 data. Specifically, trash-related complaint counts from 2022 serve as lagged data for 2023. This enables the prediction of trash complaint patterns in advance, using data available by the end of the previous year.\n\n\nCode\n# Transform 2022 data Created Date to Match Date\ncomplaints_22['Match Date'] = complaints_22['Created Date'] + pd.DateOffset(years=1)\ncomplaints_22 = complaints_22.drop(columns=['Created Date'])\n\n# Rename Count column in 2022 data\ncomplaints_22.rename(columns={\"Complaint Count\": \"Complaint Count Lag\"}, inplace=True)\n\n\n\n\nCode\n# Merge the two dataset \nDSNY_final = complaints_22.merge(\n    complaints_23,\n    left_on=['DSNY_ID', 'Match Date'],\n    right_on=['DSNY_ID', 'Created Date'],\n    how='right'\n)\n\nDSNY_final = DSNY_final.drop(columns=['Match Date', 'GEOID'])\nDSNY_final.rename(columns={'Created Date': 'Date'}, inplace=True)\n\nDSNY_final.head()\n\n\n\n\n\n\n\n\n\nDSNY_ID\nComplaint Count Lag\ngeometry\nMed_HH_Inc\nPct_Non_White\nPct_Hispanic\nPoverty_Rate\nAvg_HH_Size\nRestaurant Inspection Count\nDerelict Vehicle Count\nGraffiti Count\navg_temp\nprecipitation\nsnow\nDate\nComplaint Count\nBasket Count\nTraffic Count\nHoliday\nDay of Week\nWeek\n\n\n\n\n0\nBKN011_A\n0.0\nMULTIPOLYGON (((995020.254 205025.197, 995260....\n146250.0\n30.224904\n13.878223\n5.951728\n2.11\nNaN\nNaN\nNaN\n10.1\n31.0\n0.0\n2023-01-01\n0.0\n47\n9794.421906\nFalse\nSunday\n52\n\n\n1\nBKN011_A\n0.0\nMULTIPOLYGON (((995020.254 205025.197, 995260....\n146250.0\n30.224904\n13.878223\n5.951728\n2.11\nNaN\nNaN\nNaN\n11.5\n0.8\n0.0\n2023-01-02\n0.0\n47\n9794.421906\nFalse\nMonday\n1\n\n\n2\nBKN011_A\n1.0\nMULTIPOLYGON (((995020.254 205025.197, 995260....\n146250.0\n30.224904\n13.878223\n5.951728\n2.11\nNaN\nNaN\nNaN\n1.2\n0.0\n0.0\n2023-01-03\n0.0\n47\n9794.421906\nFalse\nTuesday\n1\n\n\n3\nBKN011_A\n0.0\nMULTIPOLYGON (((995020.254 205025.197, 995260....\n146250.0\n30.224904\n13.878223\n5.951728\n2.11\nNaN\nNaN\nNaN\n-3.8\n0.0\n0.0\n2023-01-04\n0.0\n47\n9794.421906\nFalse\nWednesday\n1\n\n\n4\nBKN011_A\n0.0\nMULTIPOLYGON (((995020.254 205025.197, 995260....\n146250.0\n30.224904\n13.878223\n5.951728\n2.11\nNaN\nNaN\nNaN\n2.5\n7.4\n0.0\n2023-01-05\n0.0\n47\n9794.421906\nFalse\nThursday\n1",
    "crumbs": [
      "Data Analysis",
      "Gather and Explore Additional Modeling Data"
    ]
  },
  {
    "objectID": "analysis/2-gather-and-explore-additional-modeling-data.html#visualizing-and-exploring-variables",
    "href": "analysis/2-gather-and-explore-additional-modeling-data.html#visualizing-and-exploring-variables",
    "title": "Gather and Explore Additional Modeling Data",
    "section": "6 Visualizing and Exploring Variables",
    "text": "6 Visualizing and Exploring Variables\nWe created a series of plots and maps below to explore the association between the dependent variable (complaint count of the predicted year) and the independent variables.\n\n6.1 Correlation Plot Across All Variables\nThe correlation plot used a binary palette to reveal the correlation between independent variable and all numeric variables. The legend indicates that 1.0 (deep red) indicates perfect positive correlation, 0 (neutral grey) indicates no correlation, and -1.0 (deep blue) indicates perfect negative correlation.\nThe chart suggests that some correlation exists between trash-related complaint count of 2023 and the following variables: - Count of complaints in the previous year (Complaint Count Lag) - Share of non-white population in the previous year (Pct_Non_White) - Share of Hispanic-Latino population in the previous year (Pct_Hispanic) - Poverty rate in the previous year (Poverty_Rate) - Average household size in the previous year (Avg_HH_Size) - Count of Restaurant Inspections in the previous year (Restaurant Inspection Count) - Count of derelict vehicles in the previous year (Derelict Vehicle Count) - Average daily temperature in the previous year (avg_temp) - Projected basket count within the DSNY boundary (basket count)\n\n\nCode\nDSNY_final = DSNY_final.apply(pd.to_numeric, errors='ignore')\n\nDSNY_final['Week'] = DSNY_final['Week'].astype(str) \n\ncorrelation_matrix = DSNY_final.corr()\n\nplt.figure(figsize=(10, 8))\n\nplt.imshow(correlation_matrix, cmap=\"coolwarm\", interpolation=\"nearest\", vmin=-1, vmax=1)\n\nplt.xticks(range(correlation_matrix.shape[1]), correlation_matrix.columns, rotation=90)\nplt.yticks(range(correlation_matrix.shape[1]), correlation_matrix.columns)\n\nplt.colorbar(label=\"Correlation Coefficient\")\n\nplt.title(\"Correlation Grid (Gradient Colors)\")\n\nplt.tight_layout()\nplt.show()\n\n\nFutureWarning: The default value of numeric_only in DataFrame.corr is deprecated. In a future version, it will default to False. Select only valid columns or specify the value of numeric_only to silence this warning.\n\n\n\n\n\n\n\n\n\n\n\n6.2 Time series of complaint count\nNext, we plotted a time series heatmap of total complaint counts by “Week” and “Day of the Week” to further analyze the temporal patterns of trash-related complaints across NYC.\nIn line with our observations, complaint counts generally increase during warmer months (particularly in summer) compared to the beginning and end of the year. However, contrary to our initial expectations, there are generally more complaints on weekdays than on weekends. Nonetheless, the heatmap clearly indicates a generally consistent weekly pattern, with certain days showing higher complaint frequencies.\n\n\nCode\nday_order = ['Sunday', 'Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday']\nDSNY_final['Day of Week'] = pd.Categorical(DSNY_final['Day of Week'], categories=day_order, ordered=True)\n\nDSNY_final_heatmap = DSNY_final.copy()\n\nDSNY_final_heatmap['Week'] = DSNY_final['Week'].astype(int)\n\nheatmap_data = DSNY_final_heatmap.pivot_table(\n    index='Week', columns='Day of Week', values='Complaint Count', aggfunc='sum', fill_value=0\n)\n\nplt.figure(figsize=(10, 8))\nsns.heatmap(\n    heatmap_data,\n    cmap='viridis', \n    annot=False,    \n    fmt=\"d\",         \n    cbar=True       \n)\nplt.title('Heatmap of Complaint Counts by Day and Week of Year', fontsize=14)\nplt.xlabel('Day of Week', fontsize=12)\nplt.ylabel('Week', fontsize=12)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n6.3 Map of Complaint Count in relation to highly-correlated factors\n\n6.3.1 Create a Mapping Master Dataframe\n\n\nCode\nDSNY_final_grouped = DSNY_final.groupby('DSNY_ID').agg({\n    'Complaint Count': 'sum',\n    'Complaint Count Lag': 'sum',         \n    'Pct_Non_White': 'mean',    \n    'Pct_Hispanic': 'mean',  \n    'Poverty_Rate': 'mean',  \n    'Avg_HH_Size': 'mean',  \n    'Restaurant Inspection Count': 'sum',\n    'Derelict Vehicle Count':'sum',\n    'Basket Count': 'mean'      \n}).reset_index()\n\n# Rename columns\nDSNY_final_grouped.rename(columns={\n    \"Complaint Count\": \"2023 Annual Total of Complaint Count\",\n    \"Complaint Count Lag\": \"2022 Annual Total of Complaint Count\",\n    \"Pct_Non_White\": \"2023 Share of Non-White Population\",\n    \"Pct_Hispanic\": \"2023 Share of Hispanic-Latino Population\",\n    \"Poverty_Rate\": \"2023 Poverty Rate\", \n    \"Avg_HH_Size\": \"2023 Average Household Size\",\n    \"Restaurant Inspection Count\": \"2023 Annual Total of Restaurant Inspection Count\",\n    \"Derelict Vehicle Count\": \"2023 Annual Total of Derelict Vehicle Count\",\n    \"Basket Count\": \"2023 Annual Total of Basket Count\"\n}, inplace=True)\n\n\n\n\nCode\nDSNY_final_grouped = DSNY_final_grouped.merge(DSNY_boundary, on=\"DSNY_ID\", how=\"right\")\nDSNY_final_grouped = gpd.GeoDataFrame(DSNY_final_grouped, geometry=DSNY_final_grouped['geometry'], crs=\"EPSG:4326\")\nDSNY_final_grouped = DSNY_final_grouped.to_crs(DSNY_boundary.crs)\n\n\n\n\n6.3.2 Mapping Dependent and Independent Variables\nThen, we created a series of maps to visualize the spatial distribution of trash-related complaints with correlated variables. The following highlights are made:\n\nSpots with high complaint counts in 2023 is generally the same as that of 2022, with hotspots stretching all the way from south of the Central Park all the way to northern Brooklyn. This indicates that lagged complaint count is a powerful predictor of the next year’s pattern.\nWhereas in other regions, complaint counts tend to correspond spatially with areas of lower non-white population share, lower Hispanic-Latino population share, and lower poverty rates, the Bronx Borough stands out as the only area where high complaint counts align with the opposite trends.\nHigher complaint counts aligns with areas with smaller average households, likely due to density of development and increased ground traffic. High complaint counts also correspond with areas with a high frequency of restaurant inspections and denser litter basket placement. This likely reflects dense business development and human activity, as seen in areas such as Midtown Manhattan.\nComplaints also tend to occur in areas with a high prevalence of derelict vehicles, except in the area spanning Midtown Manhattan to the southern tip of Manhattan, where complaints are less likely driven by a lack of management and more reflective of active business corridors.\n\n\n\nCode\nmetrics = [\"2023 Annual Total of Complaint Count\", \"2022 Annual Total of Complaint Count\", \"2023 Share of Non-White Population\", \"2023 Share of Hispanic-Latino Population\",\n           \"2023 Poverty Rate\", \"2023 Average Household Size\", \"2023 Annual Total of Restaurant Inspection Count\", \"2023 Annual Total of Derelict Vehicle Count\", \n           \"2023 Annual Total of Basket Count\"]\n\n\n\n\nCode\nnum_cols = 3\nnum_rows = -(-len(metrics) // num_cols)\n\nfig, axes = plt.subplots(num_rows, num_cols, figsize=(18, 5 * num_rows)) \n\naxes = axes.flatten()\n\n# Plot each metric\nfor i, metric in enumerate(metrics):\n    ax = axes[i]\n    DSNY_final_grouped.plot(\n        column=metric, \n        cmap=\"viridis\", \n        legend=True, \n        ax=ax,\n        legend_kwds={\n            'shrink': 0.5,  \n            'location': 'right', \n            'pad': 0.02,\n            'aspect': 20   \n        }\n    )\n    ax.set_title(f\"{metric} in NYC\", fontsize=15)\n\n# Turn off unused subplots\nfor j in range(len(metrics), len(axes)):\n    fig.delaxes(axes[j])\n\n# Adjust layout\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n6.4 Chart of Complaint Count with Natural Factors\nFinally, we analyzed the relationship between trash-related complaints and daily temperature. Consistent with our observations from the time series heatmap, we found a positive correlation between higher complaint volumes and higher daily temperatures.\n\n\nCode\nplt.figure(figsize=(10, 6))\n\nsns.regplot(\n    data=DSNY_final,\n    x='Complaint Count',        \n    y='avg_temp',\n    scatter_kws={'alpha': 0.5},  \n    line_kws={'color': 'red'},   \n    ci=95                       \n)\n\nplt.title('Complaint Count vs. Daily Temperature', fontsize=14)\nplt.xlabel('Complaint Count', fontsize=12)\nplt.ylabel('Daily Temperature (°F)', fontsize=12)\n\n\nText(0, 0.5, 'Daily Temperature (°F)')",
    "crumbs": [
      "Data Analysis",
      "Gather and Explore Additional Modeling Data"
    ]
  },
  {
    "objectID": "analysis/2-gather-and-explore-additional-modeling-data.html#save-data-for-model-building",
    "href": "analysis/2-gather-and-explore-additional-modeling-data.html#save-data-for-model-building",
    "title": "Gather and Explore Additional Modeling Data",
    "section": "7 Save Data for Model Building",
    "text": "7 Save Data for Model Building\n\n\nCode\nDSNY_master = DSNY_final.drop(columns=[\"geometry\"])\n\nDSNY_master.to_parquet('data/2_DSNY_master.parquet', index=False)",
    "crumbs": [
      "Data Analysis",
      "Gather and Explore Additional Modeling Data"
    ]
  },
  {
    "objectID": "analysis/1-process-and-visualize-sanitation-request-data.html",
    "href": "analysis/1-process-and-visualize-sanitation-request-data.html",
    "title": "Process and Visualize Sanitation Request Data",
    "section": "",
    "text": "This section merges sanitation requests with DSNY trash-removal unit boundaries, and uses panel to visualize the temporal and spatial distribution of requests between 2022 and 2023.",
    "crumbs": [
      "Data Analysis",
      "Process and Visualize Sanitation Request Data"
    ]
  },
  {
    "objectID": "analysis/1-process-and-visualize-sanitation-request-data.html#set-up",
    "href": "analysis/1-process-and-visualize-sanitation-request-data.html#set-up",
    "title": "Process and Visualize Sanitation Request Data",
    "section": "1 Set Up",
    "text": "1 Set Up\n\n\nCode\nimport pandas as pd\nimport geopandas as gpd\nimport numpy as np\nimport datetime as dt\n\nimport folium\nimport xyzservices\nimport matplotlib.pyplot as plt\n\n# Load panel and enable interactive features\nimport panel as pn\n\npn.extension()",
    "crumbs": [
      "Data Analysis",
      "Process and Visualize Sanitation Request Data"
    ]
  },
  {
    "objectID": "analysis/1-process-and-visualize-sanitation-request-data.html#merging-complaint-and-spatial-data",
    "href": "analysis/1-process-and-visualize-sanitation-request-data.html#merging-complaint-and-spatial-data",
    "title": "Process and Visualize Sanitation Request Data",
    "section": "2 Merging Complaint and Spatial Data",
    "text": "2 Merging Complaint and Spatial Data\nPoint-level 311 sanitation complaints and DSNY units are merged to glimpse into the temporal and spatial distribution of the requests. To prepare the dataset, we loaded the 311 complaints and DSNY trash removal units, and reprojected them to the New York state plane before completing the merging.\nDSNY defines its operational unit at multiple tiers. Areas in New York City is categorized into district, section, and frequency boundaries in descending order. For this report, we define DSNY unit per its definition of frequncy boundaries, which are the actual operational units for trash collection. Per this definition, there are 609 DSNY units in total. To distinguish and target those units, we created a new column “DSNY_ID” in the original boundary dataset.\nsource: DSNY Frequencies [https://data.cityofnewyork.us/City-Government/DSNY-Frequencies/rv63-53db/about_data]\n\n2.1 Load trimmed NYC 311 Complaints\n\n\nCode\n# Load the Parquet file\ncomplaints = pd.read_parquet('data/complaint_filtered.parquet')\n\n\n\n\nCode\n# Filter information needed\ncomplaints_cols = ['Unique Key', 'Complaint Type', 'Location Type', 'Latitude', 'Longitude', 'Created Date']\ncomplaints = complaints.loc[:, complaints.columns.isin(complaints_cols)]\n\n\n\n\n2.2 Load boundaries of New York City Department of Sanitation (DSNY)\n\n\nCode\nDSNY_boundary = gpd.read_file('data/DSNY Boundaries.geojson')\n\n# reproject the boundaries\nDSNY_boundary = DSNY_boundary.to_crs(epsg=2263)\n\n\n\n\nCode\nDSNY_boundary['DSNY_ID'] = DSNY_boundary['section'].astype(str) + \"_\" + DSNY_boundary['frequency'].astype(str)\n\n# Filter information needed\nDSNY_boundary = DSNY_boundary[['DSNY_ID', 'geometry']]\n\n\n\n\n2.3 Save Processed DSNY Boundary Data\nDo not re-run：This chunk is used solely for data storage purposes. \n\n\nCode\nDSNY_boundary.to_file(\"data/1_DSNY_boundary_processed.geojson\", driver=\"GeoJSON\")\n\n\n\n\n2.4 Merge sanitation complaint data with DSNY boundaries\n\n\nCode\n# Convert the complaints data to a GeoDataFrame\ncomplaints_gdf = gpd.GeoDataFrame(\n  complaints, geometry=gpd.points_from_xy(complaints.Longitude, complaints.Latitude), crs=\"EPSG:4326\"\n  )\n\n# Convert the complaints data to the CRS EPSG=2263\ncomplaints_gdf = complaints_gdf.to_crs(epsg=2263)\ncomplaints_gdf[\"Unique Key\"] = complaints_gdf[\"Unique Key\"].astype(str)\n\n\n\n\nCode\n# Perform the spatial join\ncomplaints_DSNY = gpd.sjoin(\n    complaints_gdf, DSNY_boundary, how=\"right\", predicate=\"within\").reset_index(drop=True)\n\n# Drop the 'index_left' column\ncomplaints_DSNY = complaints_DSNY.drop(columns=['index_left'])",
    "crumbs": [
      "Data Analysis",
      "Process and Visualize Sanitation Request Data"
    ]
  },
  {
    "objectID": "analysis/1-process-and-visualize-sanitation-request-data.html#visualizations-using-panel",
    "href": "analysis/1-process-and-visualize-sanitation-request-data.html#visualizations-using-panel",
    "title": "Process and Visualize Sanitation Request Data",
    "section": "3 Visualizations using panel",
    "text": "3 Visualizations using panel\nWe created a panel tool to display sanitation-related complaints by type and date across all 609 DSNY units. While the actual panels are too large to be incorporated to this site, we encourage the users referring to “analysis/1-process-visualize-sanitation-request.ipynb” to construct the panels locally and interact with the tool.\nIn both years, the quantity of illegal dumping and dirty conditions significantly exceed litter basket complaint and dumpster complaint. Spatially, trash related complaints tend to cluster in Manhattan, Bronx, and Brooklyn, as compared to the other two boroughs. For Queens Borough, complaints tend to cluster in the south end as compared to the north.\nComplaints also vary temporally. For instance, in 2022, illegal dumping becomes more frequeny in warmer seasons (the second and third quarter), and declines in colder seasons (the first and fourth quarter). This likely reflects lower temperature’s discouraging effect towards human activities and trashing.\n\n3.1 Create a widget and a slider for filtering the sanitation complaints by type and date\n\n\nCode\n# Create the widget for different complaint types\ncomplaint_types = list(complaints_DSNY['Complaint Type'].dropna().unique())\n\ncomplaint_type_select = pn.widgets.Select(\n    value=\"Litter Basket Complaint\", options=complaint_types, name=\"Complaint Type\"\n)\n\n\n\n\nCode\n# Create time sliders for 2022 and 2023 separately\nDEFAULT_START_1 = dt.datetime(2022, 1, 1)\nDEFAULT_END_1 = dt.datetime(2022, 12, 31)\n\nDEFAULT_START_2 = dt.datetime(2023, 1, 1)\nDEFAULT_END_2 = dt.datetime(2023, 12, 31)\n\ndatetime_range_slider_2022 = pn.widgets.DatetimeRangeSlider(\n    name=\"Date Slider 2022\",\n    start=DEFAULT_START_1,\n    end=DEFAULT_END_1,\n    value=(DEFAULT_START_1, DEFAULT_END_1),\n    step=1000 * 60 * 60 * 24,  # 1 day in milliseconds\n)\n\ndatetime_range_slider_2023 = pn.widgets.DatetimeRangeSlider(\n    name=\"Date Slider 2023\",\n    start=DEFAULT_START_2,\n    end=DEFAULT_END_2,\n    value=(DEFAULT_START_2, DEFAULT_END_2),\n    step=1000 * 60 * 60 * 24,  # 1 day in milliseconds\n)\n\n\n\n\n3.2 Define functions to filter the data based on the complaint type and date\n\n\nCode\n# Define a function to filter the complaints data by complaint type\ndef filter_by_complaint_type(data, complaint_type):\n    \"\"\"\n    Filter data by complaint type.\n    \"\"\"\n    sel = data[\"Complaint Type\"] == complaint_type\n    return data.loc[sel]\n\n# Define a function to filter the complaints data by date range\ndef filter_by_date(data, start_date, end_date):\n    \"\"\"\n    Filter data by start and end date.\n    \"\"\"\n    # Convert complaints date column of strings to DateTime objects\n    created_date = pd.to_datetime(data[\"Created Date\"])\n\n    ## Two selections\n    sel_start = created_date &gt;= start_date\n    sel_end = created_date &lt;= end_date\n\n    ## Logical \"and\" for final selection\n    date_sel = sel_start & sel_end\n\n    ## Filter complaints by date range\n    return data.loc[date_sel]\n\n\n\n\n3.3 Define a function to plot the filtered complaints data and the DSNY boundaries\n\n\nCode\ndef plot_complaint_data(data, DSNY_boundary):\n    \"\"\"\n    Plot the complaint data and DSNY boundary on a Folium map.\n    \"\"\"\n    # Plot the DSNY boundary\n    DSNY_boundary = DSNY_boundary.to_crs(epsg=4326)\n\n    m = DSNY_boundary.explore(\n        style_kwds={\"weight\": 0.5, \"color\": \"gray\", \"fillColor\": \"none\"},\n        name=\"DSNY boundary\",\n        tiles=xyzservices.providers.CartoDB.Voyager,\n    )\n\n    complaints_df = data.drop(columns='geometry')\n    complaints_df = complaints_df.dropna(subset=['Longitude', 'Latitude'])\n    complaints_df['Created Date'] = complaints_df['Created Date'].astype(str)\n\n    points = gpd.GeoDataFrame(\n        complaints_df,\n        geometry=gpd.points_from_xy(complaints_df['Longitude'], complaints_df['Latitude']),\n        crs=\"EPSG:4326\",\n    )\n\n    # Overlay all points as circle markers on the map\n    points.explore(\n        m=m,\n        marker_kwds={\"radius\": 2, \"fill\": True, \"color\": \"crimson\"},\n        marker_type=\"circle_marker\",  # Options: 'circle_marker', 'marker'\n        name=\"Complaints\",\n    )\n\n    return m\n\n\n\n\n3.4 Define a function to create the dashboard for visualizing complaints data\n\n\nCode\ndef create_dashboard(complaint_type, date_range, DSNY_boundary):\n    \"\"\"\n    Plot trash-related requests for the input neighborhood.\n    \"\"\"\n    # Step 1: Filter the complaints data by input complaint type and time\n    complaints = filter_by_complaint_type(complaints_DSNY, complaint_type)\n    complaints = filter_by_date(complaints, date_range[0], date_range[1])\n\n    # Step 2: Make the Folium map\n    m = plot_complaint_data(complaints, DSNY_boundary)\n\n    # Step 3: Return a Folium pane\n    return pn.pane.plot.Folium(m, height=600)\n\n\n\n\n3.5 Create the dashboard for 2022 complaints data\nThe dashboard generated by panel is unable to display in the website rendered by quarto and also too large to be uploaded to github if deployed. Please refer to and download from complaints dashboard 2022 [https://drive.google.com/file/d/1_Ao6Zl_NZtixNh9ZPs9ACPVRR6Jr8Jo1/view?usp=sharing] for the panel visualization.\n\n\nCode\n# Create the dashboard for 2022\ncomplaint_dashboard_2022 = pn.Column(\n    # Top: the title and dropdown widget\n    pn.Column(\n        \"## Sanitation-Related 311 Tickets by DSNY Boundary - 2022\",\n        complaint_type_select,\n        datetime_range_slider_2022,\n    ),\n    # Add a height spacer\n    pn.Spacer(height=25),\n    # Bottom: the main chart, bind widgets to the function\n    pn.bind(\n        create_dashboard,\n        DSNY_boundary=DSNY_boundary,\n        complaint_type=complaint_type_select,\n        date_range=datetime_range_slider_2022,\n    ),\n)\n\ncomplaint_dashboard_2022.save(\"complaint_dashboard_2022.html\", embed=True)\n# complaint_dashboard_2022 # hidden to reduce notebook size\n\n\n                                                                                                                       \n\n\n\n\n\nComplaint Dashboard 2022: Litter Basket Complaint\n\n\n\n\n\nComplaint Dashboard 2022: Illegal Dumping\n\n\n\n\n3.6 Create the dashboard for 2023 complaints data\nThe dashboard generated by panel is unable to display in the website rendered by quarto and also too large to be uploaded to github if deployed. Please refer to and download from complaints dashboard 2023 [https://drive.google.com/file/d/1QsHRyEgokmOp9xgAQjU-84xuU6cgTu3n/view?usp=sharing] for the panel visualization.\n\n\nCode\n# Create the dashboard for 2023\ncomplaint_dashboard_2023 = pn.Column(\n    # Top: the title and dropdown widget\n    pn.Column(\n        \"## Sanitation-Related 311 Tickets by DSNY Boundary - 2023\",\n        complaint_type_select,\n        datetime_range_slider_2023,\n    ),\n    # Add a height spacer\n    pn.Spacer(height=25),\n    # Bottom: the main chart, bind widgets to the function\n    pn.bind(\n        create_dashboard,\n        DSNY_boundary=DSNY_boundary,\n        complaint_type=complaint_type_select,\n        date_range=datetime_range_slider_2023,\n    ),\n)\n\ncomplaint_dashboard_2023.save(\"complaint_dashboard_2023.html\", embed=True)\n# complaint_dashboard_2023 # hidden to reduce notebook size\n\n\n                                                                                                                       \n\n\n\n\n\nComplaint Dashboard 2023: Illegal Dumping\n\n\n\n\n\nComplaint Dashboard 2023: Dumpster Complaint",
    "crumbs": [
      "Data Analysis",
      "Process and Visualize Sanitation Request Data"
    ]
  },
  {
    "objectID": "analysis/1-process-and-visualize-sanitation-request-data.html#further-process-and-save-data-for-later-use",
    "href": "analysis/1-process-and-visualize-sanitation-request-data.html#further-process-and-save-data-for-later-use",
    "title": "Process and Visualize Sanitation Request Data",
    "section": "4 Further process and save data for later use",
    "text": "4 Further process and save data for later use\nPer the goal of the final product, we grouped and counted the total trash-related complaints for each DSNY boundary by day. This dataframe is saved for later analysis use.\n\n4.1 Group complaints by district and date\n\n\nCode\ncomplaints_DSNY_grouped = (\n    complaints_DSNY.groupby(['DSNY_ID', 'Created Date'])['Unique Key']\n    .nunique()\n    .reset_index()\n)\n\ncomplaints_DSNY_grouped.rename(columns={'Unique Key': 'Complaint Count'}, inplace=True)\n\n\n\n\n4.2 Add missing dates to district when no complaints is filled\nCreating a full DSNY-unit/date panel: Acknowleding records are missing for DSNY units on the day where no complaints are filed, we created a full DSNY-unit/date panel, to obtain a complete dataframe where no records is coded as zero. This approach ensures a comprehensive dataset that captures both active complaint days and periods of no activity, enabling more accurate analysis.\n\n\nCode\n# Generate the full date range\ndate_range = pd.date_range(start=\"2022-01-01\", end=\"2023-12-31\")\n\n# Get unique DSNY units\nunique_IDs = DSNY_boundary['DSNY_ID'].unique()\n\n# Create a dataframe with all DSNY unit-date combinations\nfull_panel = pd.MultiIndex.from_product(\n    [unique_IDs, date_range], names=['DSNY_ID', 'Created Date']\n).to_frame(index=False)\n\n# Merge the full data with the original dataframe\ncomplaints_DSNY_grouped['Created Date'] = pd.to_datetime(complaints_DSNY_grouped['Created Date'], errors='coerce')\n\ncomplaints_DSNY_full = pd.merge(\n    full_panel,\n    complaints_DSNY_grouped,\n    on=['DSNY_ID', 'Created Date'],\n    how='left'\n)\n\n# Fill missing complaint count with 0\ncomplaints_DSNY_full['Complaint Count'] = complaints_DSNY_full['Complaint Count'].fillna(0)\n\n# Sort by unit and date\ncomplaints_DSNY_full = complaints_DSNY_full.sort_values(by=['DSNY_ID', 'Created Date']).reset_index(drop=True)\n\n\n\n\n4.3 Split and Save 2022 and 2023 data\n\n\nCode\n# Filter for 2022\ncomplaints_DSNY_full_2022 = complaints_DSNY_full[complaints_DSNY_full['Created Date'].dt.year == 2022]\n\n# Filter for 2023\ncomplaints_DSNY_full_2023 = complaints_DSNY_full[complaints_DSNY_full['Created Date'].dt.year == 2023]\n\n\n\n\nCode\ncomplaints_DSNY_full_2022.to_parquet('data/1_complaints_DSNY_full_2022.parquet', index=False)\n\ncomplaints_DSNY_full_2023.to_parquet('data/1_complaints_DSNY_full_2023.parquet', index=False)",
    "crumbs": [
      "Data Analysis",
      "Process and Visualize Sanitation Request Data"
    ]
  },
  {
    "objectID": "analysis/0a-create-311-parquet.html",
    "href": "analysis/0a-create-311-parquet.html",
    "title": "Create 311 Parquet",
    "section": "",
    "text": "Do not Re-run!!\nIn this section, we inspected and cleaned New York City’s 311 Service Requests dataset (from 2010 to present) with 38.5 million rows. The steps include loading and converting the raw data to a master parquet tile, investigating the 311 records, filtering the dataset based on trash-related complaint type, trimming to the specific time range between 2022-01-01 and 2023-12-31, and saving the processed records.\nThis section should not be re-run due to its sole purpose for the creation of more manageable dataset.\nsource: 311 Service Requests from 2010 to Present, NYC Open Data [https://data.cityofnewyork.us/Social-Services/311-Service-Requests-from-2010-to-Present/erm2-nwe9/about_data]",
    "crumbs": [
      "Data Analysis",
      "Create 311 Parquet"
    ]
  },
  {
    "objectID": "analysis/0a-create-311-parquet.html#set-up",
    "href": "analysis/0a-create-311-parquet.html#set-up",
    "title": "Create 311 Parquet",
    "section": "1 Set Up",
    "text": "1 Set Up\n\nimport pandas as pd\nimport geopandas as gpd\nimport numpy as np\n\nfrom datetime import datetime\nimport matplotlib.pyplot as plt",
    "crumbs": [
      "Data Analysis",
      "Create 311 Parquet"
    ]
  },
  {
    "objectID": "analysis/0a-create-311-parquet.html#process-master-parquet-file",
    "href": "analysis/0a-create-311-parquet.html#process-master-parquet-file",
    "title": "Create 311 Parquet",
    "section": "2 Process Master Parquet File",
    "text": "2 Process Master Parquet File\nWe downloaded the 311 request dataset with 38.5 million rows from NYC Open Data, and stored it as a parquet to reduce file size and enhance usability.\n\n# Create the master Parquet file\ncsv_file = \"data/311_Service_Requests.csv\"\nparquet_file = \"311_Service_Requests.parquet\"\n\ndf = pd.read_csv(csv_file)\n\ndf['Incident Zip'] = df['Incident Zip'].astype(pd.StringDtype())\n\ndf.to_parquet('311_Service_Requests.parquet', engine='pyarrow', compression='snappy', index=False)\n\nC:\\Users\\ztyuu\\AppData\\Local\\Temp\\ipykernel_35184\\48896794.py:5: DtypeWarning: Columns (8,17,18,20,31,32) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(csv_file)",
    "crumbs": [
      "Data Analysis",
      "Create 311 Parquet"
    ]
  },
  {
    "objectID": "analysis/0a-create-311-parquet.html#trim-to-the-responsible-agency",
    "href": "analysis/0a-create-311-parquet.html#trim-to-the-responsible-agency",
    "title": "Create 311 Parquet",
    "section": "3 Trim to the Responsible Agency",
    "text": "3 Trim to the Responsible Agency\nAll trash complaints are reported to New York Department of Sanitation (DSNY). Therefore, in this step, we trimmed the dataset to reflect only those handled by DSNY.\n\n# Load the Master Parquet file\nrequests = pd.read_parquet('data/311_Service_Requests.parquet')",
    "crumbs": [
      "Data Analysis",
      "Create 311 Parquet"
    ]
  },
  {
    "objectID": "analysis/0a-create-311-parquet.html#filter-complaints-by-type",
    "href": "analysis/0a-create-311-parquet.html#filter-complaints-by-type",
    "title": "Create 311 Parquet",
    "section": "4 Filter Complaints by Type",
    "text": "4 Filter Complaints by Type\nThen, we inspected all unique complaint types handled by DSNY, and used a histogram to understand major types of trash-related complaints.\n\nfiltered_requests = requests[\n    (requests['Agency'] == 'DSNY')]\n\n# Display unique categories in the \"Complaint Type\" column\ncomplaint_types = filtered_requests['Complaint Type'].unique()\n\n\n# Count number of complaint submitted to DSNY\ncomplaint_counts = filtered_requests['Complaint Type'].value_counts()\n\n# Plot histogram\nplt.figure(figsize=(12, 8))\ncomplaint_counts.plot(kind='bar', color='skyblue')\nplt.title('Frequency of Complaint Types (DSNY)', fontsize=16)\nplt.xlabel('Complaint Type', fontsize=14)\nplt.ylabel('Frequency', fontsize=14)\nplt.xticks(rotation=90, fontsize=12)\nplt.tight_layout()  # Adjust layout to prevent overlap\nplt.show()",
    "crumbs": [
      "Data Analysis",
      "Create 311 Parquet"
    ]
  },
  {
    "objectID": "analysis/0a-create-311-parquet.html#filter-complaints-by-type-1",
    "href": "analysis/0a-create-311-parquet.html#filter-complaints-by-type-1",
    "title": "Create 311 Parquet",
    "section": "5 Filter Complaints by Type",
    "text": "5 Filter Complaints by Type\nBased on earlier inspections, we devised a list of complaint types representing most popular trash-removal requests. We further filtered those records from the dataset.\n\ncomplaint_list = ['Dirty Conditions',\n                  'Sanitation Condition',\n                  'Dirty Condition',\n                  'Illegal Dumping',\n                  'Overflow Litter Baskets',\n                  'Litter Basket Complaint',\n                  'Dumpster Complaint',\n                  'Overflowing Recycling Baskets']\n\n# Filter for a specific complaint type and date range\ncomplaint_filtered = filtered_requests[\n    (filtered_requests['Complaint Type'].isin(complaint_list))    \n]",
    "crumbs": [
      "Data Analysis",
      "Create 311 Parquet"
    ]
  },
  {
    "objectID": "analysis/0a-create-311-parquet.html#filter-complaints-by-time",
    "href": "analysis/0a-create-311-parquet.html#filter-complaints-by-time",
    "title": "Create 311 Parquet",
    "section": "6 Filter Complaints by Time",
    "text": "6 Filter Complaints by Time\nFinally, we trimmed the range of the filtered records down to 2022-01-01 to 2023-12-31.\n\n# Create a column of the complaint creation date\ncomplaint_filtered = complaint_filtered.rename(columns={'Created Date': 'Created Date Time'})\ncomplaint_filtered = complaint_filtered.rename(columns={'Closed Date': 'Closed Date Time'})\n\ncomplaint_filtered['Created Date'] = pd.to_datetime(complaint_filtered['Created Date Time'], format='%m/%d/%Y %I:%M:%S %p').dt.date\n\n\nstart_date = datetime.strptime('2022-01-01', '%Y-%m-%d').date()\nend_date = datetime.strptime('2023-12-31', '%Y-%m-%d').date()\n\n# Filter the dataFrame to desired time range\ncomplaint_filtered = complaint_filtered[\n    (complaint_filtered['Created Date'] &gt;= start_date) & \n    (complaint_filtered['Created Date'] &lt;= end_date)\n]\n\ncomplaint_filtered = complaint_filtered.drop(columns=['Created Date Time', 'Closed Date Time'])",
    "crumbs": [
      "Data Analysis",
      "Create 311 Parquet"
    ]
  },
  {
    "objectID": "analysis/0a-create-311-parquet.html#create-the-trimmed-parquet-file",
    "href": "analysis/0a-create-311-parquet.html#create-the-trimmed-parquet-file",
    "title": "Create 311 Parquet",
    "section": "7 Create the Trimmed Parquet File",
    "text": "7 Create the Trimmed Parquet File\n\n# Create the trimmed parquet file - do not re-run!!!\n\ncomplaint_filtered.to_parquet('complaint_filtered.parquet', engine='pyarrow', compression='snappy', index=False)\n\nprint(f\"CSV converted to Parquet\")\n\nCSV converted to Parquet",
    "crumbs": [
      "Data Analysis",
      "Create 311 Parquet"
    ]
  },
  {
    "objectID": "conclusion.html",
    "href": "conclusion.html",
    "title": "Conclusion",
    "section": "",
    "text": "From both analytical and application-driven perspectives, the project is a preliminary step to investigating and improving trash removal services in New York City.\n\nAnalytical Findings\nSummarizing distribution of trash-related sanitation complaints (Section “Gather and Explore Additional Modeling Data”), our study has found consistent clustering patterns from Midtown Manhattan to northern Brooklyn. In most of the boroughs, higher complaint counts spatially corresponded with areas of lower non-white population share, lower Hispanic-Latino population share, and lower poverty rates; the Bronx Borough, however, stands out as the only area where high complaint counts align with the opposite trends.\nWe also note the dilemma that complaints may occur in places across the spectrum of differing management levels: on one hand, areas with denser urban development - indicated by smaller household units, vibrant food and drink services, as well as better litter basket placement - can be prone to frequent complaints due to high human activities; on the other hand, complaints also occur in vacant or under-maintained areas, such as those with high prevalence of derelict vehicles. This observation clearly indicates a potential to further distinguish different types of trash-related complaints, and implement different resources tailored to unique neighborhoods’ characteristics and challenges.\n\n\nApplication-driven Results\nUsing Poisson Regression, we were able to deliver a data-driven complaint count projection tool designed to facilitate future scheduling needs of the New York City Department of Sanitation (DSNY). Overall, the model predicts better than taking the mean or median or historic data, and accommodates data release dates to allow prediction one-year ahead.\nBy applying the model to predict complaint counts for 2024, we generated daily projections for each DSNY unit throughout the year. These projections enable several practical applications, such as identifying the day of the week with the highest number of complaints city-wide, targeting the day of the week with the highest complaints within each DSNY unit, and locating the top ten DSNY units with the highest overall complaints. These insights can assist in optimizing trash collection schedules and improving route planning to enhance operational efficiency.\n\n\nModel Improvement Suggestions\nNonetheless, as the current model is trained on open data, the inherent limitations such as lack of details or historic data lead to two limitations in the model that will benefit from future improvement.\nFirst, when comparing the model’s results to actual 2023 data, we observed a tendency of the model to overpredict in areas like Midtown Manhattan, the Bronx, parts of central Brooklyn, and southern Queens, where urban developments are denser and the original complaint counts are higher. In contrast, lower errors are observed in regions like southern Brooklyn and Staten Island, indicating better alignment between predicted and actual counts. These findings highlight the model’s limitations in capturing the dynamics of denser, high-activity urban zones.\nSecond, while the model provides temporal predictions, the day-to-day differences within most DSNY units are not significantly distinct. As many DSNY units share the same peak trash removal day of the week (Thursday), this could complicate task scheduling and hinder strategic resource allocation.\nTo tackle the two shortcomings, we recommend incorporating more granular spatial data or leveraging additional time-series information to enhance the model’s accuracy and temporal differentiation. We welcome feedback and collaboration opportunities to refine and further develop this project. If you are interested, please feel free to reach out to us!",
    "crumbs": [
      "Conclusion"
    ]
  }
]